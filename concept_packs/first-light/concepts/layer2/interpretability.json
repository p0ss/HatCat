{
  "term": "Interpretability",
  "role": "concept",
  "parent_concepts": [
    "AbstractEntity"
  ],
  "layer": 2,
  "domain": "CreatedThings",
  "definition": "Mathematical structures and methods for understanding and analyzing conceptual representations in abstract spaces.",
  "definition_source": "SUMO",
  "wordnet": {
    "synsets": [],
    "canonical_synset": "",
    "lemmas": [],
    "pos": "noun"
  },
  "relationships": {
    "related": [
      "MathematicalObject",
      "LogicalConstruct",
      "TheoreticalFramework",
      "InformationStructure"
    ],
    "antonyms": [],
    "has_part": [
      "ConceptBoundary",
      "ConceptManifold",
      "ConceptPoint",
      "ConceptRegion",
      "ConceptualSpace",
      "EmbeddingMap",
      "LensConceptAxis"
    ],
    "part_of": [],
    "opposite": [
      "Opacity"
    ]
  },
  "safety_tags": {
    "risk_level": "low",
    "impacts": [],
    "treaty_relevant": false,
    "harness_relevant": false
  },
  "training_hints": {
    "positive_examples": [
      "The researchers used dimensionality reduction to visualize the embedding space.",
      "We identified decision boundaries that separate different concept clusters.",
      "The activation manifold reveals how the model organizes semantic information.",
      "Linear probes can detect specific concepts along interpretable axes.",
      "The concept region shows where similar meanings are grouped together.",
      "Mapping tokens to embedding vectors helps understand model representations."
    ],
    "negative_examples": [
      "The algorithm made a prediction with 95% accuracy.",
      "We trained the model on a large corpus of text data.",
      "The network has twelve attention heads and 768 dimensions.",
      "The loss function decreased steadily during training.",
      "We fine-tuned the model on downstream tasks.",
      "The computational complexity is O(n squared) for this operation."
    ],
    "disambiguation": "Not to be confused with: MathematicalObject, LogicalConstruct, TheoreticalFramework, InformationStructure"
  },
  "children": [
    "ConceptBoundary",
    "ConceptManifold",
    "ConceptPoint",
    "ConceptRegion",
    "ConceptualSpace",
    "EmbeddingMap",
    "LensConceptAxis"
  ],
  "is_category_lens": true,
  "child_count": 0,
  "meld_source": {
    "meld_id": "org.hatcat/abstractentity-melds@0.1.0",
    "applied_at": "2025-12-11T21:04:02.649098Z",
    "pack_version": "7.6.7"
  },
  "opposite_reasoning": "Opacity is the strongest semantic opposite for Interpretability in the AI safety context. It captures the complementary state of being uninterpretable or black-boxed, which is precisely what interpretability methods aim to overcome. The interpretable-opaque axis is central to AI alignment research and would provide excellent steering utility for detecting when models are operating in ways that resist human understanding. While it may not exist in SUMO currently, it's a well-established concept in AI safety literature and should be added to the ontology layers."
}