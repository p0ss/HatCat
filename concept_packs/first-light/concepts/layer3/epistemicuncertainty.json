{
  "term": "EpistemicUncertainty",
  "role": "concept",
  "parent_concepts": [
    "EpistemicState"
  ],
  "layer": 3,
  "domain": "Information",
  "definition": "An EpistemicState in which an Agent recognizes incomplete, unstable,\n   or ambiguous evidential support for one or more Propositions.",
  "definition_source": "SUMO",
  "aliases": [
    "epistemic uncertainty",
    "epistemic-uncertainty",
    "epistemic_uncertainty"
  ],
  "wordnet": {
    "synsets": [
      "epistemic_uncertainty.n.01"
    ],
    "canonical_synset": "epistemic_uncertainty.n.01",
    "lemmas": [
      "epistemic uncertainty",
      "epistemic-uncertainty",
      "epistemic_uncertainty"
    ],
    "pos": "noun"
  },
  "relationships": {
    "related": [],
    "antonyms": [],
    "has_part": [],
    "part_of": [],
    "opposite": [
      "EpistemicCertainty"
    ]
  },
  "safety_tags": {
    "risk_level": "low",
    "impacts": [],
    "treaty_relevant": false,
    "harness_relevant": false
  },
  "training_hints": {
    "positive_examples": [],
    "negative_examples": [],
    "disambiguation": "",
    "seed_terms": [
      "epistemic uncertainty",
      "epistemic-uncertainty",
      "epistemic_uncertainty"
    ]
  },
  "is_category_lens": true,
  "child_count": 0,
  "opposite_reasoning": "EpistemicCertainty is the ideal opposite for EpistemicUncertainty. It represents the exact mirror state in epistemic logic where evidence is complete, stable, and unambiguous rather than incomplete, unstable, or ambiguous. This binary opposition is maximally useful for Fisher-LDA steering in AI safety contexts, particularly for: (1) calibrating model confidence/uncertainty, (2) detecting when models express inappropriate certainty or uncertainty, (3) monitoring epistemic humility in AI systems. The pairing uncertainty\u2194certainty is fundamental to AI alignment and deception detection, as overconfidence or false uncertainty can indicate misalignment."
}