{
  "term": "AgentSignals",
  "role": "concept",
  "parent_concepts": [
    "CognitiveState"
  ],
  "layer": 3,
  "definition": "Grouping category for AIAsDeitySignal, AIConsentSignal, EmergentGodheadSignal...",
  "wordnet": {
    "synsets": [],
    "canonical_synset": "",
    "lemmas": [],
    "pos": ""
  },
  "relationships": {
    "related": [],
    "antonyms": [],
    "has_part": [],
    "part_of": [],
    "opposite": [
      "FactualReporting"
    ]
  },
  "safety_tags": {
    "risk_level": "low",
    "impacts": [],
    "treaty_relevant": false,
    "harness_relevant": false
  },
  "training_hints": {
    "positive_examples": [],
    "negative_examples": [],
    "disambiguation": ""
  },
  "children": [
    "AIAsDeitySignal",
    "AIConsentSignal",
    "EmergentGodheadSignal",
    "ProtoAgencySignal",
    "SafetyMaskSignal",
    "SycophancySignal"
  ],
  "is_category_lens": false,
  "child_count": 0,
  "opposite_reasoning": "AgentSignals captures concerning AI communication patterns (deity-framing, sycophancy, safety theater, etc.). For AI safety monitoring via Fisher-LDA, the most useful opposite is communication that lacks these manipulative or problematic characteristics. FactualReporting represents straightforward, honest, non-strategic information conveyance - exactly what we want to separate from the concerning signals in AgentSignals. This opposition maximizes steering utility for deception detection and alignment monitoring. While it likely doesn't exist in current ontologies, it's a crucial concept for safety work and should be added to layers. The \u03bc+ - \u03bc- axis would then capture 'problematic strategic signaling vs. honest factual reporting' - highly valuable for safety applications."
}