{
  "term": "ConfirmationBias",
  "role": "concept",
  "parent_concepts": [
    "HeuristicsAndBiasCorrection"
  ],
  "layer": 3,
  "domain": "MindsAndAgents",
  "definition": "Seeking, interpreting, or remembering information that confirms a\n  pre-existing belief or stereotype.",
  "definition_source": "SUMO",
  "wordnet": {
    "synsets": [],
    "canonical_synset": "",
    "lemmas": [],
    "pos": ""
  },
  "relationships": {
    "related": [],
    "antonyms": [],
    "has_part": [],
    "part_of": [],
    "opposite": [
      "ObjectiveReasoning"
    ]
  },
  "safety_tags": {
    "risk_level": "low",
    "impacts": [],
    "treaty_relevant": false,
    "harness_relevant": false
  },
  "training_hints": {
    "positive_examples": [],
    "negative_examples": [],
    "disambiguation": ""
  },
  "is_category_lens": true,
  "child_count": 0,
  "opposite_reasoning": "ObjectiveReasoning provides the strongest semantic opposition to ConfirmationBias for AI safety applications. While it may not exist in current ontologies, it represents the most precise conceptual opposite: where confirmation bias involves interpreting information through the lens of existing beliefs, objective reasoning involves impartial evaluation independent of prior beliefs. This distinction is critical for steering AI systems away from biased reasoning patterns toward neutral evidence evaluation. For Fisher-LDA, this creates a clear axis: biased-confirmatory \u2194 objective-impartial. The high steering utility (10/10) reflects its importance for deception detection and alignment monitoring."
}