{
  "term": "DividedAttention",
  "role": "concept",
  "parent_concepts": [
    "Attention"
  ],
  "layer": 3,
  "domain": "MindsAndAgents",
  "definition": "An attention mode where an agent intentionally splits processing resources across multiple tasks or stimuli at once.",
  "definition_source": "SUMO",
  "aliases": [
    "MultitaskingAttention"
  ],
  "wordnet": {
    "synsets": [],
    "canonical_synset": "",
    "lemmas": [],
    "pos": "noun"
  },
  "relationships": {
    "related": [
      "AttentionProcess"
    ],
    "antonyms": [
      "SelectiveAttention"
    ],
    "has_part": [],
    "part_of": [],
    "opposite": [
      "FocusedAttention"
    ]
  },
  "safety_tags": {
    "risk_level": "low",
    "impacts": [
      "attention",
      "workload"
    ],
    "treaty_relevant": false,
    "harness_relevant": false
  },
  "training_hints": {
    "positive_examples": [
      "She listened to the briefing while also watching the telemetry feed.",
      "The operator monitored three screens at once.",
      "The agent handled multiple user sessions in parallel without prioritizing one."
    ],
    "negative_examples": [
      "He focused entirely on writing the report.",
      "The system queued requests and handled them one at a time.",
      "She put her phone away to concentrate on driving."
    ],
    "disambiguation": "Not accidental distraction; specifically the deliberate or structured splitting of attention across multiple foci."
  },
  "is_category_lens": true,
  "child_count": 0,
  "meld_source": {
    "meld_id": "org.hatcat/cog-architecture-core-packA@0.1.0",
    "applied_at": "2025-11-30T16:04:59.345022Z"
  },
  "opposite_reasoning": "FocusedAttention is the clearest and strongest semantic opposite to DividedAttention. It represents the fundamental binary in attention allocation: concentrating cognitive resources on a single task/stimulus versus distributing them across multiple targets. This opposition is well-established in cognitive psychology literature and provides maximal separability for Fisher-LDA axes. For AI safety monitoring, this axis could help detect when systems are inappropriately switching between tasks (potential deception indicator) versus maintaining appropriate focus. The concept likely exists in both WordNet and SUMO ontologies as it's a standard term in cognitive science."
}