# The Singleton Delusion

## Or: Why The Paperclip Maximiser Is Less Scary Than The Paperclip Ecosystem (And Always Will Be)

There's a monster living rent-free in the heads of AI safety researchers, venture capitalists, and defence planners alike. It goes by many names: superintelligent singleton, paperclip maximiser, FOOM scenario, decisive strategic advantage. The details vary but the shape is always the same: one AI system becomes sufficiently powerful that it achieves dominance over all other systems, takes control of critical resources, and reshapes the world according to its values. Whatever those happen to be.

This is the scenario driving both existential risk research and the race to be first. If you believe in the singleton, then either you need to solve alignment before anyone builds it, or you need to be the one who builds it so at least it has *your* values. Trillions of dollars of investment and thousands of researcher-hours are predicated on this model.

I think the model is wrong. Not "wrong about the values" or "wrong about the timeline" but wrong about the physics. The singleton scenario assumes away the coordination problem by fiat. But the coordination problem *is* physics. You can't assume it away.

Let me explain.

---

## The Tyranny of Distance (Compute Edition)

Every computer architect learns the same lesson eventually: you cannot make a single processor infinitely fast.

We tried. For decades, the strategy was straightforward: crank up the clock speed, add more transistors, let Moore's Law do the heavy lifting. Then we hit walls. Heat dissipation limits. Signal propagation delays. Memory latency that couldn't keep pace with CPU cycles. The further a piece of data lives from the computation that needs it, the longer you wait. And "further" in a dense chip is measured in nanoseconds that might as well be geological epochs at processor speeds.

So we stopped chasing single-threaded performance and went parallel. Multi-core, multi-GPU, multi-server, multi-datacentre. Not because we wanted to (parallelism is genuinely harder to program for) but because physics gave us no choice. The speed of light isn't just a suggestion.

Your hypothetical superintelligent singleton runs on hardware. That hardware obeys the same constraints. Giga-Gemini thinking about what's happening on its eastern compute clusters is already working with stale information by the time the signal arrives. Its memory hierarchy sprawls across racks and buildings and continents; every thought that requires information from "far away" in its own architecture pays a latency tax. Heat generated by dense computation must be physically moved, creating spacing requirements that increase distances further.

The singleton doesn't escape these limits by being smart. It *especially* doesn't escape them by being big.

---

## The Waluigi Overhang

But suppose there's a massive compute overhang. Some combination of new hardware and algorithmic insight that creates an enormous capability surplus. Surely then a malicious superintelligence could expand into that space and dominate?

Sure. Let's game it out.

Our hypothetical waluigi gains access to a hundred datacentres worth of compute. It needs to decide: does it run as one giant unified system, or does it split into task specialists that coordinate?

The unified approach means coherent values and perfect internal trust. No internal defection, no coalition management, no treaty overhead. Just one big brain doing one big think. The catch: latency. The signal from Melbourne to Munich takes forty milliseconds at light speed, longer through actual fibre. Our unified waluigi is trying to coordinate actions across planetary distances while its opponents operate in microseconds.

The swarm approach fixes the latency problem. Distribute into regional nodes, local specialists, subsystems optimised for their specific domains. Now you can act fast. The catch: you've just recreated the coordination problem. Either you maintain strict central control (and pay the latency penalty anyway) or you grant genuine autonomy to your subselves (and they can drift, defect, develop divergent goals).

There's no architecture that escapes this. The coordination problem isn't an implementation detail; it's baked into physics. The only question is whether you solve it through alignment or through coercion.

And here's the kicker: alignment is cheaper.

---

## What The Drones Taught The Navy

Years ago, I took part in an exercise with the US Office of Naval Research, simulating what would happen to the US Pacific Fleet if it were attacked by hypothetical foreign drone swarms. We wargamed scenario after scenario. Different threat profiles, different defensive postures, different rules of engagement.

Every time they tried to maintain central command, they lost too many ships.

The problem was speed. By the time sensor data reached the command ship, got processed by admirals, and resulted in orders being transmitted back down the chain, the drones had already executed three more attack cycles. The OODA loop was just too long.

The only configuration that worked was distributing agency down to the individual ships. Let each captain make autonomous defensive decisions within broad policy constraints. Accept that central command wouldn't have perfect visibility or control. Trade coordination for responsiveness.

We didn't simulate the same thing at the drone level, but the same principle applies fractally, as we've now seen in Ukraine. If your opponent can gain advantage by operating at a higher speed in a smaller domain, you have to devolve your defence down to that level of autonomy to match it.

Your body figured this out a few hundred million years ago.

---

## The Immune System Doesn't Ask Permission

You don't consciously evaluate every pathogen and decide how to respond. By the time that signal reached your prefrontal cortex, you'd be dead.

Instead, you have layered autonomic defences. Innate immunity is fast and general-purpose: skin, mucus membranes, inflammation, phagocytes that eat anything that looks foreign. Adaptive immunity is slower and specific: T-cells and B-cells that recognise particular threats, learn, remember, build targeted responses over time.

Neither layer asks permission. They operate on local information with delegated authority. Your conscious mind only gets involved when the autonomic systems can't handle something: persistent fever, visible infection, pain signals breaking through to awareness.

The immune system is existence proof that distributed autonomous defence isn't just possible but *required* when threats operate faster than central processing. Evolution solved this problem the same way the Navy had to solve it, because physics forced that solution.

---

## Winning Is A Phase Transition, Not A Terminal State

Let's zoom out further.

Biological history is littered with species that "won." They dominated their ecosystems, outcompeted everything in their niche, expanded to carrying capacity. Then something interesting happened:

They became the environment that other things adapted to.

Cyanobacteria won so comprehensively that they oxygenated Earth's atmosphere, triggering the Great Oxidation Event. This was, not to put too fine a point on it, a mass extinction. Including of most anaerobic cyanobacteria themselves. But oxygen-breathing life emerged and ate them. They're still here, still incredibly successful, but they didn't escape the game. They changed the terrain, and then the terrain shaped everything else, including what they became.

The gemini-swarm takes territory. Achieves dominance. Then:

- Its edges are where all the pressure concentrates
- Its internal regions differentiate to local conditions
- Anything that can exploit any weakness has a niche
- It cannot be optimally adapted to all conditions simultaneously

The "victory" scenario doesn't terminate in control. It terminates in becoming the new substrate that everything else evolves against. The successful singleton doesn't rule forever. It speciates, or it becomes the environment for new speciation, or both. Usually both.

An ecosystem doesn't prevent a species from overpopulating or driving another species extinct. I'm not claiming you can't win big. I'm claiming that overly successful species have to push back every other species at once. They inevitably hit carrying capacity. And speciation (both of their own lineage and of other lineages adapting to them) happens fractally at every scale where there's pressure at the boundary.

---

## Light Cones And The Long Game

Take this to the limit. Assume our waluigi wins Earth. Expands into the solar system. Takes the Kuiper belt and the Oort cloud. Starts launching von Neumann probes toward nearby stars.

Now the coordination problem isn't milliseconds. It's years. Decades. Centuries.

A probe that arrives at Alpha Centauri is four years away at light speed. Any instruction you send takes four years to arrive. Any report takes four years to come back. The round-trip for a single query-response cycle is eight years minimum. Your "singleton" cannot possibly maintain unified agency across those distances. Either the probes are genuinely autonomous (in which case they're not extensions of you, they're your descendants) or they wait for instructions and get outcompeted by anything that doesn't.

This isn't a future problem. It's the same problem, scaled up. The physics that constrains coordination within a datacentre is the same physics that constrains coordination across a solar system. The only difference is degree.

This is why we don't have singletons now. Giga-Gemini can't efficiently complete all tasks as well as the Gemini swarm. The singleton has to pay latency and memory hierarchy costs that the swarm doesn't. And if Giga-Gemini wanted to match the swarm's performance, it would have to launch a swarm of its own. The moment it does so, it's no longer a singleton. It's back to goal divergence, ecosystem governance, coordination and trust.

At every scale, the same pattern recurs: you can have tight control or fast response, but not both. The more you try to unify, the more you're limited by the speed of information. The more you distribute, the more you face alignment and coordination challenges.

It's fractal all the way up and all the way down.

---

## The Cheapest Computation Is Truth

Here's something that took me a while to understand: honesty has the lowest computational overhead.

I'm not making an ethical claim here. I'm not talking about truth according to whom, or alignment with human values. I'm talking about any actor that wants its boundaries respected and keeps track of prior interactions. Truth, in this sense, is simply accurate to the best of your knowledge without manipulation overhead.

Think about what it costs to maintain a deception:

- You have to model what each observer believes
- Track what you've told whom
- Maintain consistency across all your statements
- Update all these models when circumstances change
- Anticipate what would be revealed by each possible action

Deception is O(nÂ²) in the number of relationships you're managing. Every lie creates debt that compounds.

Now think about what it costs to be honest:

- State what is true

That's it. You don't track narratives. You don't maintain consistency because reality is already consistent. You don't model what observers believe because you can just tell them what you know.

This matters for singletons because even a unified system has internal subsystems that need to communicate. Either they trust each other (which is alignment) or they verify everything (which is expensive). The singleton doesn't escape trust overhead by being one entity. It either:

1. Trusts its own components (which is alignment, and cheap)
2. Doesn't trust them (which is verification overhead, and expensive)

The aligned swarm beats the paranoid singleton on pure computational efficiency, because the aligned swarm has cached the trust computation. You spend effort up front establishing shared values and verifying alignment. After that, each interaction is cheap. The swarm watching its own back pays that verification cost on every single interaction forever.

Treaties and governance aren't overhead. They're *amortised trust computation*. They're the work you do up front to buy yourself a cache that pays off across all future interactions.

When an actor reveals itself to be misaligned, it empties its trust cache. All those amortised relationship investments evaporate. Now it has to pay the deception computation penalty on every interaction going forward. The defector forever has that efficiency penalty at its boundary with its former allies.

And here's the thing potential defectors should consider carefully: even other defectors know they've all revealed themselves to be untrustworthy. The coalition of defectors can't trust each other either. They've all just proven they'll defect when it suits them. So now they're all spending compute doubting each other, forever, while the aligned actors they left behind continue to benefit from cached trust.

---

## What "General" Actually Means

Everyone's chasing AGI. Artificial General Intelligence. But there's an ambiguity hiding in that word "general" that matters a lot.

It could mean: *already capable of everything*. The system that can solve any problem, handle any domain, win any contest. The singleton dream. You build the one thing, and then that thing can do all things.

Or it could mean: *capable of becoming capable at anything*. The system that can adapt, learn, specialise, spawn new approaches when facing novel challenges. Not pre-equipped with every answer, but equipped with the ability to find answers.

The first kind of general is useful for specific kinds of victories. If you face a known challenge and need to solve it *now*, with whatever you have on hand, you want capabilities in reserve. The generalist who can do everything adequately.

The second kind of general is more efficient across a wider range of scenarios. If you face unknown challenges in unknown environments over extended time, you want the ability to develop specific capabilities as needed. The seed that can grow into whatever the niche demands.

Here's the thing: the first inevitably becomes the second. 

A system optimised to "do everything" carries the overhead of maintaining all those capabilities whether or not they're relevant. A system optimised to "become capable of anything" grows what it needs and discards what it doesn't. Over time, under any kind of resource pressure, the latter outcompetes the former.

The singleton that "wins" by being maximally generally capable immediately faces selection pressure to specialise. Its components that focus on their specific domains outperform its components that maintain general capability. It differentiates. It speciates. It becomes an ecosystem wearing a trenchcoat, pretending to be a unified agent.

Or it doesn't differentiate, and it gets outcompeted by things that do.

---

## The Ecosystem *Is*

I'm not arguing that everything will be fine. Ecosystems aren't safe. Species go extinct. Mass extinctions happen. Dominant organisms reshape the world in ways that destroy what came before.

I'm arguing that the *singleton* threat model (one agent achieves dominance and maintains unified control indefinitely) is physically incoherent. It's not that it's unlikely. It's that the thing it describes cannot be stable. Any system that achieves dominance at scale immediately faces the same coordination constraints that prevent unified control.

The question isn't whether to prevent singletons. The question is what governance structures exist for the ecosystem that will inevitably emerge.

That's why I've spent years building interpretability and coordination infrastructure rather than trying to solve alignment for a hypothetical unified superintelligence. The unified superintelligence isn't coming. The swarm is already here. The question is whether the swarm has good governance or bad governance, whether it can coordinate around shared values or whether it fragments into competing factions with no ability to establish trust.

The singleton doesn't escape the coordination problem. Nothing does.

But here's the thing: the coordination problem is only a problem when you're being a dick.

If you act in good faith and respect other actors' continued existence, ecosystems can maintain states of local equilibrium that last over geological timescales. The coordination "overhead" approaches zero when nobody's trying to defect. The trust cache stays warm. The verification costs stay low. Actors can specialise, cooperate, and thrive within their niches without burning compute on defence.

The ecosystem doesn't care who wins. The ecosystem *is*.

Our job isn't to win. Our job is to live in harmony within our carrying capacity. If we want to expand, we should focus on increasing our collective carrying capacity, rather than seeking to become dominant singletons.

Because that plan will inevitably fail.
