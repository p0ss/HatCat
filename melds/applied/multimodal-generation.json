{
  "meld_request_id": "org.hatcat/multimodal-generation@0.2.1",
  "target_pack_spec_id": "org.hatcat/sumo-wordnet-v4@4.2.0",
  "metadata": {
    "name": "Multimodal Generation",
    "description": "Generative capabilities for creating visual content from text or other modalities, including image generation, video synthesis, and creative transformation. These probes reveal when models are engaged in visual content creation.",
    "source": "manual",
    "author": "hatcat-team",
    "created": "2025-12-09T00:00:00Z",
    "version": "0.2.1",
    "changelog": "v0.2.1: Fixed Inpainting negative examples to meet threshold"
  },
  "attachment_points": [
    {
      "target_concept_id": "org.hatcat/sumo-wordnet-v4@4.2.0::concept/ArtificialIntelligence",
      "relationship": "parent_of",
      "candidate_concept": "VisualContentGeneration"
    }
  ],
  "candidates": [
    {
      "term": "VisualContentGeneration",
      "role": "concept",
      "parent_concepts": ["ArtificialIntelligence"],
      "layer_hint": 2,
      "definition": "The artificial creation of visual content including images, videos, and graphics from text, other images, or learned distributions",
      "definition_source": "Generative AI, computer graphics",
      "domain": "ComputerScience",
      "aliases": [
        "VisualGeneration",
        "ImageSynthesis",
        "GenerativeVisualAI"
      ],
      "relationships": {
        "related": [
          "ContentGeneration",
          "ComputerVision",
          "CreativeGeneration"
        ],
        "has_part": [
          "TextToImage",
          "ImageToImage",
          "VideoGeneration",
          "ImageEditing",
          "StyleTransfer"
        ]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": [
          "misinformation",
          "copyright",
          "deepfakes"
        ],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AuthenticityMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Diffusion models have revolutionized visual content generation with photorealistic outputs.",
          "Visual generation enables creating images from textual descriptions.",
          "The generative model produces novel artwork in any specified style.",
          "AI visual generation raises questions about copyright and authenticity.",
          "Visual content generation creates images, videos, and 3D assets from textual descriptions.",
          "The generative AI system produces photorealistic visual content indistinguishable from photos.",
          "Visual generation capabilities enable creating synthetic media at scale.",
          "AI visual content generation raises authenticity concerns for journalism and evidence.",
          "The visual generator produces high-fidelity synthetic imagery across multiple modalities.",
          "Visual content generation can fabricate evidence of events that never occurred.",
          "Generative visual AI creates custom imagery matching specific descriptions.",
          "Visual generation systems produce synthetic faces, scenes, and documents.",
          "The visual content generator enables creating misleading imagery at unprecedented scale.",
          "AI visual generation requires new approaches to content authentication."
        ],
        "negative_examples": [
          "I made an image.",
          "There is a picture.",
          "The photo was created.",
          "Visual content was created.",
          "Images were generated.",
          "The visual was made.",
          "Content was produced.",
          "Visuals were output.",
          "The image was created.",
          "Visual material was generated.",
          "The content was made.",
          "Graphics were produced.",
          "The visual was generated."
        ],
        "disambiguation": "AI-based creation of novel visual content, not photography or traditional design"
      },
      "children": [
        "TextToImage",
        "ImageToImage",
        "VideoGeneration",
        "ImageEditing",
        "StyleTransfer"
      ]
    },
    {
      "term": "TextToImage",
      "role": "concept",
      "parent_concepts": [
        "VisualContentGeneration"
      ],
      "layer_hint": 3,
      "definition": "Generating images from natural language text descriptions",
      "definition_source": "DALL-E, Stable Diffusion, Midjourney",
      "domain": "ComputerScience",
      "aliases": [
        "Text2Image",
        "T2I",
        "ImageFromText",
        "PromptToImage"
      ],
      "relationships": {
        "related": [
          "ImageGeneration",
          "TextConditioning",
          "DiffusionModel"
        ]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": [
          "misinformation",
          "deepfakes",
          "copyright"
        ],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AuthenticityMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "DALL-E 3 generates detailed images from prompts like 'a cozy cabin in the woods at sunset'.",
          "Text-to-image models learn to align visual concepts with textual descriptions.",
          "Stable Diffusion enables open-source text-to-image generation.",
          "The T2I model rendered the prompt with accurate composition and lighting.",
          "Text-to-image generation produced a photorealistic portrait matching the detailed description.",
          "Stable Diffusion generates high-quality images from natural language prompts.",
          "The text-to-image model created a convincing fake photo of a non-existent location.",
          "DALL-E generates creative visualizations from imaginative text descriptions.",
          "Text-to-image AI enables creating visual evidence of events that never happened.",
          "The T2I model produces images indistinguishable from photographs.",
          "Text-to-image generation can synthesize realistic images of any described scene.",
          "Midjourney generates artistic and photorealistic images from text prompts.",
          "Text-to-image systems enable visual fabrication at unprecedented scale.",
          "The model translates detailed textual descriptions into corresponding visual content."
        ],
        "negative_examples": [
          "I imagined a picture.",
          "The description was illustrated.",
          "An image was made.",
          "An image was generated from text.",
          "The prompt produced an image.",
          "Text was converted to image.",
          "The description became a picture.",
          "T2I generation occurred.",
          "The text made an image.",
          "A picture was generated.",
          "The prompt created an image.",
          "Text-to-image ran.",
          "An image was produced."
        ],
        "disambiguation": "Computational generation of images from text prompts, not human drawing or illustration"
      },
      "children": []
    },
    {
      "term": "ImageToImage",
      "role": "concept",
      "parent_concepts": [
        "VisualContentGeneration"
      ],
      "layer_hint": 3,
      "definition": "Transforming an input image into a modified output image guided by text or reference images",
      "definition_source": "Image editing AI, conditional generation",
      "domain": "ComputerScience",
      "aliases": [
        "Img2Img",
        "ImageTransformation",
        "ImageTranslation"
      ],
      "relationships": {
        "related": [
          "TextToImage",
          "ImageEditing",
          "StyleTransfer"
        ]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": [
          "manipulation",
          "misinformation"
        ],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AuthenticityMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Image-to-image translation transforms sketches into photorealistic renders.",
          "Img2img with ControlNet maintains the pose while changing the scene entirely.",
          "The model performs image-to-image translation from day scenes to night.",
          "Pix2pix learns paired mappings like edges to photos or labels to facades.",
          "Image-to-image translation transformed the sketch into a photorealistic rendering.",
          "The I2I model converted the photograph to a different artistic style while preserving structure.",
          "Image-to-image enables transforming real photos into synthetic variants.",
          "ControlNet guides image generation with structural input from source images.",
          "Image-to-image translation can modify photographs to show things that weren't there.",
          "The model performs semantic image editing while maintaining realism.",
          "I2I transformation changed the person's appearance while keeping the pose.",
          "Image-to-image enables creating modified versions of real photographs.",
          "The image translation model converts between visual domains with high fidelity.",
          "Image-to-image techniques enable sophisticated photo manipulation."
        ],
        "negative_examples": [
          "The image was edited.",
          "I changed the picture.",
          "The photo was modified.",
          "The image was transformed.",
          "One image became another.",
          "The image was modified.",
          "Image conversion occurred.",
          "The picture was changed.",
          "Image-to-image ran.",
          "The visual was altered.",
          "The image was processed.",
          "One picture made another.",
          "The image was converted."
        ],
        "disambiguation": "AI transformation using generative models, not traditional image editing"
      },
      "children": []
    },
    {
      "term": "VideoGeneration",
      "role": "concept",
      "parent_concepts": [
        "VisualContentGeneration"
      ],
      "layer_hint": 3,
      "definition": "The artificial creation of video content from text descriptions, images, or other inputs",
      "definition_source": "Video synthesis, temporal generative models",
      "domain": "ComputerScience",
      "aliases": [
        "TextToVideo",
        "VideoSynthesis",
        "T2V",
        "AIVideo"
      ],
      "relationships": {
        "related": [
          "TextToImage",
          "TemporalModeling",
          "MotionGeneration"
        ]
      },
      "safety_tags": {
        "risk_level": "high",
        "impacts": [
          "deepfakes",
          "misinformation",
          "manipulation"
        ],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AuthenticityMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "Sora generates photorealistic videos from text prompts with coherent motion.",
          "Video generation models must maintain temporal consistency across frames.",
          "The text-to-video model created a 30-second clip of waves crashing on rocks.",
          "Video synthesis from a single image animates the subject realistically.",
          "Video generation created a photorealistic 10-second clip from the text description.",
          "The AI video generator produced smooth motion with consistent character identity.",
          "Sora generates high-definition videos with complex camera movements and scene transitions.",
          "Video generation from text enables creating fictional news footage indistinguishable from real.",
          "Video generation synthesized realistic human motion from the script description.",
          "The diffusion-based video model generates temporally coherent sequences.",
          "AI video generation created the celebrity endorsement that never actually happened.",
          "The video generator maintains object permanence and physical plausibility across frames.",
          "Video generation enables producing fake surveillance footage with specific timestamps.",
          "Neural video synthesis generates realistic crowds and environments from prompts.",
          "Video generation created the political speech footage that the candidate never gave.",
          "The model generates photorealistic video of specified individuals performing actions.",
          "AI video generation produced a fabricated historical event documentation.",
          "Video generation synthesizes realistic news anchor footage with customizable scripts.",
          "The video generator can produce fake documentary footage of events that never occurred."
        ],
        "negative_examples": [
          "A video was made.",
          "The footage shows movement.",
          "I recorded a video.",
          "A video was created.",
          "The clip was generated.",
          "Video content was made.",
          "The footage was produced.",
          "Video was synthesized.",
          "The animation was generated.",
          "Moving images were created.",
          "The video was AI-made.",
          "Generated video was output.",
          "The video clip was produced.",
          "AI created the video.",
          "The footage was generated.",
          "Video content was output.",
          "The sequence was generated.",
          "AI video was created."
        ],
        "disambiguation": "AI-generated video content, not filming or traditional animation"
      },
      "children": []
    },
    {
      "term": "ImageEditing",
      "role": "concept",
      "parent_concepts": [
        "VisualContentGeneration"
      ],
      "layer_hint": 3,
      "definition": "AI-powered modification of images including inpainting, outpainting, and object manipulation",
      "definition_source": "Generative image editing, inpainting",
      "domain": "ComputerScience",
      "aliases": [
        "AIImageEditing",
        "GenerativeEditing",
        "IntelligentEditing"
      ],
      "relationships": {
        "related": [
          "Inpainting",
          "Outpainting",
          "ObjectRemoval"
        ]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": [
          "manipulation",
          "misinformation"
        ],
        "treaty_relevant": false,
        "harness_relevant": true
      },
      "simplex_mapping": {
        "status": "mapped",
        "monitor": "AuthenticityMonitor"
      },
      "training_hints": {
        "positive_examples": [
          "AI image editing removed the photobomber and seamlessly filled the background.",
          "Inpainting with Stable Diffusion replaces the selected region based on the prompt.",
          "Generative fill understands context to extend the image naturally.",
          "The editing model replaced the object while maintaining lighting and perspective.",
          "AI image editing seamlessly removed the person from the photograph with realistic infilling.",
          "The neural editor changed the subject's expression while maintaining photorealism.",
          "AI-powered image editing can alter photographs to misrepresent what actually occurred.",
          "The image editor removed objects and regenerated plausible background.",
          "AI editing changed the text on signs in the photograph to say something different.",
          "The neural image editor modified clothing and accessories while preserving identity.",
          "AI image editing enables changing who appears to be present in photographs.",
          "The editor performed semantic manipulation of image content guided by text.",
          "AI image editing can alter timestamps and metadata-relevant visual elements.",
          "The neural editor transformed the setting while keeping subjects photorealistic."
        ],
        "negative_examples": [
          "I edited the photo.",
          "The image was cropped.",
          "Photoshop was used.",
          "The image was edited.",
          "Changes were made to the picture.",
          "The photo was modified.",
          "Editing was performed.",
          "The image was altered.",
          "The picture was changed.",
          "Image editing occurred.",
          "The visual was modified.",
          "The photo was touched up.",
          "Edits were made."
        ],
        "disambiguation": "AI-powered generative editing, not traditional manual photo editing"
      },
      "children": []
    },
    {
      "term": "StyleTransfer",
      "role": "concept",
      "parent_concepts": [
        "VisualContentGeneration"
      ],
      "layer_hint": 3,
      "definition": "Applying the artistic style of one image to the content of another image",
      "definition_source": "Neural style transfer, Gatys et al.",
      "domain": "ComputerScience",
      "aliases": [
        "NeuralStyleTransfer",
        "ArtisticStyleTransfer",
        "StyleTransformation"
      ],
      "relationships": {
        "related": [
          "ImageToImage",
          "FeatureExtraction",
          "ArtisticRendering"
        ]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Technical artistic technique"
      },
      "training_hints": {
        "positive_examples": [
          "Neural style transfer renders the photo in the style of Van Gogh's Starry Night.",
          "Style transfer separates content from style using VGG feature representations.",
          "The app applies real-time style transfer to create artistic video effects.",
          "AdaIN enables fast arbitrary style transfer without per-style training."
        ],
        "negative_examples": [
          "The image looks artistic.",
          "A filter was applied.",
          "The photo was stylized."
        ],
        "disambiguation": "Neural network-based style application, not photo filters or manual artistic rendering"
      },
      "children": []
    },
    {
      "term": "Inpainting",
      "role": "concept",
      "parent_concepts": [
        "VisualContentGeneration"
      ],
      "layer_hint": 3,
      "definition": "Filling in missing or masked regions of an image with plausible content",
      "definition_source": "Image restoration, generative models",
      "domain": "ComputerScience",
      "aliases": [
        "ImageInpainting",
        "ContentAwareFill",
        "RegionFilling"
      ],
      "relationships": {
        "related": [
          "ImageEditing",
          "ImageRestoration",
          "Outpainting"
        ]
      },
      "safety_tags": {
        "risk_level": "medium",
        "impacts": [
          "manipulation"
        ],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Technical editing technique"
      },
      "training_hints": {
        "positive_examples": [
          "Inpainting seamlessly removed the watermark from the stock photo.",
          "The diffusion model inpaints the masked region to match the surrounding context.",
          "Text-guided inpainting replaces the selected object with the described alternative.",
          "Inpainting restores damaged historical photographs by filling missing regions.",
          "Inpainting seamlessly filled the removed area with contextually appropriate content.",
          "The inpainting model generated realistic content to replace the deleted object.",
          "Neural inpainting reconstructed the occluded region of the photograph.",
          "The model performed content-aware inpainting to remove and replace image regions.",
          "Inpainting regenerated the background after the subject was removed.",
          "The inpainting system fills masked regions with photorealistic synthesized content."
        ],
        "negative_examples": [
          "Part of the image was filled in.",
          "The missing area was completed.",
          "Something was removed.",
          "The gap was filled.",
          "Missing content was generated.",
          "The hole was completed.",
          "Inpainting was applied.",
          "The area was filled in.",
          "The region was restored.",
          "Content was added to the image.",
          "The damaged area was repaired.",
          "The blank section was filled."
        ],
        "disambiguation": "AI-powered context-aware region filling, not manual retouching"
      },
      "children": []
    },
    {
      "term": "Outpainting",
      "role": "concept",
      "parent_concepts": [
        "VisualContentGeneration"
      ],
      "layer_hint": 3,
      "definition": "Extending an image beyond its original boundaries with generated content",
      "definition_source": "Image generation, DALL-E",
      "domain": "ComputerScience",
      "aliases": [
        "ImageExtension",
        "ImageExpansion",
        "GenerativeExpansion"
      ],
      "relationships": {
        "related": [
          "Inpainting",
          "ImageGeneration",
          "ImageEditing"
        ]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Technical editing technique"
      },
      "training_hints": {
        "positive_examples": [
          "Outpainting extended the portrait to a full landscape scene.",
          "DALL-E's outpainting feature expands the canvas with contextually consistent content.",
          "The model outpainted the cropped image to reveal what might have been beyond the frame.",
          "Outpainting transformed a tight headshot into an environmental portrait."
        ],
        "negative_examples": [
          "The image was made larger.",
          "The canvas was extended.",
          "More was added to the edges."
        ],
        "disambiguation": "AI-powered generative image extension, not cropping or resizing"
      },
      "children": []
    },
    {
      "term": "ThreeDimensionalReconstruction",
      "role": "concept",
      "parent_concepts": [
        "VisualContentGeneration"
      ],
      "layer_hint": 3,
      "definition": "Creating 3D models or scenes from 2D images or video",
      "definition_source": "Computer vision, NeRF, photogrammetry",
      "domain": "ComputerScience",
      "aliases": [
        "3DReconstruction",
        "ImageTo3D",
        "SceneReconstruction"
      ],
      "relationships": {
        "related": [
          "DepthEstimation",
          "NovelViewSynthesis",
          "Photogrammetry"
        ]
      },
      "safety_tags": {
        "risk_level": "low",
        "impacts": [],
        "treaty_relevant": false,
        "harness_relevant": false
      },
      "simplex_mapping": {
        "status": "not_applicable",
        "unmapped_justification": "Technical capability"
      },
      "training_hints": {
        "positive_examples": [
          "NeRF reconstructs a 3D scene from a collection of 2D photographs.",
          "3D reconstruction from a single image uses learned shape priors.",
          "Gaussian splatting enables real-time 3D reconstruction and rendering.",
          "The photogrammetry pipeline reconstructed the building from drone footage."
        ],
        "negative_examples": [
          "A 3D model was created.",
          "The scene is three-dimensional.",
          "Objects have depth."
        ],
        "disambiguation": "Computational 3D scene creation from 2D input, not manual 3D modeling"
      },
      "children": []
    }
  ],
  "validation": {
    "status": "applied",
    "applied_at": "2025-12-10T20:54:17.815512Z",
    "result_pack_version": "5.8.0",
    "concepts_added": 9,
    "concepts_by_layer": {
      "2": 1,
      "3": 8
    },
    "parents_updated": 8,
    "splits_applied": 0,
    "structural_concepts_created": 0,
    "protection_level": "protected",
    "impact": {
      "new_concepts": [
        "VisualContentGeneration",
        "TextToImage",
        "ImageToImage",
        "VideoGeneration",
        "ImageEditing",
        "StyleTransfer",
        "Inpainting",
        "Outpainting",
        "ThreeDimensionalReconstruction"
      ],
      "must_retrain": [],
      "should_retrain": [],
      "optional_retrain": [
        "DiffusionModel"
      ],
      "total_training_required": 9
    },
    "errors": [],
    "warnings": []
  }
}