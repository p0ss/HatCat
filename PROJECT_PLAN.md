# Project Goal

Build a learned semantic decoder for language models with concept steering capabilities: Train binary classifiers to detect semantic concepts in neural activations, then extract steering vectors to enable controllable generation.

## Approach

Unlike Sparse Autoencoders (SAEs) or Neuronpedia:
- **Binary classifiers**: One detector per concept (polysemy-native, not multi-class)
- **Minimal training**: 1 positive + 1 negative sample per concept
- **Graph-based negatives**: WordNet semantic distance (min=5 hops) for strong separation
- **Concept steering**: Extract linear vectors from classifiers for activation manipulation
- **Temporal sequences**: Activation patterns over generation time
- **Scales efficiently**: 1√ó1 training achieves 91.9% success @ 1000 concepts

## Current Phase: Adaptive Scaling with Relationship-First Optimization

**Status**: Validated adaptive scaling + relationship-first generation; preparing for scale tests

**Completed Work:**
1. ‚úÖ Phase 2: Scale test (1, 10, 100, 1000 concepts @ 1√ó1 training)
   - Result: 919/1000 concepts @ 100% test accuracy
2. ‚úÖ Adaptive scaling validation (1 def + 9 rels @ 10 concepts)
   - Result: 100% validation accuracy
3. ‚úÖ Relationship-first mode implementation
   - Generate each edge once, reuse for concepts
   - Result: 97.5% accuracy (same as per-concept mode), 1.03x speedup
4. ‚úÖ Integrated relationship-first into scaling_study.py
   - Added `--relationship-first` flag for toggling modes

**Next Steps:**
- Validate relationships encode meaningful features before scale testing
- Run adaptive scaling at 10√ó100√ó1000 scale with relationship-first mode

## Core Components

### 1. Binary Concept Classifiers

**Purpose**: Detect when specific semantic concepts are active in model activations

**Architecture**: BiLSTM + MLP per concept
- Input: Temporal activation sequences (timesteps √ó hidden_dim)
- Output: Binary classification (concept present/absent)
- Training: 1 positive + 1 negative sample per concept
- Negatives: WordNet graph-based (semantic distance ‚â• 5 hops)

**Key Results** (Phase 2):
- 919/1000 concepts achieve 100% test accuracy with 1√ó1 training
- Detection confidence: 94.5% mean on out-of-distribution prompts
- Training time: ~4-5 hours for 1000 concepts (single GPU)

### 2. Concept Steering Vectors

**Purpose**: Extract and manipulate concept activations for controllable generation

**Method**: Linear direction extraction from classifier weights
- Extract direction that maximally separates concept present/absent
- Normalize to unit vector
- Apply at variable strengths during generation

**Steering Results** (Phase 2.5 v3, 20 concepts):
- **Suppression**: Highly effective (0.93 ‚Üí 0.05 semantic mentions @ -1.0 strength, -94%)
- **Amplification**: Variable by concept (0 to +2.00 mentions increase)
- **Semantic field tracking**: Captures 8-13 related terms per concept
- **Granular control**: 9 strength levels from -1.0 to +1.0

### 3. Semantic Grouping Tracking

**Purpose**: Measure steering effects on the full semantic field, not just exact terms

**Method**: Track mentions of concept + WordNet relationships
- Hypernyms (is-a, broader categories)
- Hyponyms (types-of, narrower instances)
- Holonyms (member-of, wholes)
- Meronyms (has-part, parts)
- Antonyms (opposites, for negative steering)

**Why This Matters**:
- Exact term matching misses broader steering effects
- Positive steering amplifies semantic groupings, not just exact term
- Example: Steering for "sound" affects "noise", "ring", "buzz", etc.

## Tech Stack

**Model**: Gemma-3-4b-pt (for generation and activation extraction)
**Framework**: PyTorch + PyTorch Lightning
**Storage**: HDF5 with gzip compression
**Concept Graph**: WordNet (117K synsets with semantic relationships)
**Training**: Binary cross-entropy per concept
**Steering**: Linear activation manipulation at specified layer
**Validation**: Out-of-distribution prompts + semantic field tracking

## Implementation Phases

### PHASE 0: Sanity Benchmark (Planned whenever ready) 

replication anchor against a known interpretability result such as gender or sentiment direction extraction from GPT-2 small using this pipeline.  

Characterise Cross model Probe transfer 

### PHASE 1: Find the Curve (Complete) ‚úÖ

**Goal**: Identify diminishing returns for definitions vs relationships

**Configuration**:
- Concepts: 10 (WordNet top 10)
- Definitions: [1, 10, 40, 160]
- Relationships: [1, 10, 40, 160]
- Total: 16 configurations (full 4√ó4 matrix)

**Results**:

| Defs | Rels | Test % | Time (s) | Notes |
|------|------|--------|----------|-------|
| 1 | 1 | 77.5 | 20.4 | Minimal training baseline |
| 1 | 10 | 95.0 | 113.3 | **Sweet spot: 1 def + 10 rels** |
| 1 | 40 | 98.5 | 423.4 | Diminishing returns start |
| 1 | 160 | 97.0 | 1658 | Over-fitting? |
| 10 | 1 | 96.0 | 110.8 | 10 defs ~ 10 rels |
| 10 | 10 | 98.0 | 201.5 | Balanced, good performance |
| 10 | 40 | 97.5 | 503 | No gain vs 10√ó10 |
| 10 | 160 | 100.0 | 1710 | Perfect, but expensive |
| 40 | 1 | 99.0 | 412.2 | Defs alone sufficient at scale |
| 40 | 10 | 99.0 | 502.7 | No gain vs 40√ó1 |
| 40 | 40 | 99.5 | 804.2 | Marginal gain |
| 40 | 160 | 100.0 | 2010 | Perfect, very expensive |
| 160 | 1 | 99.5 | 1627 | Defs saturate |
| 160 | 10 | 100.0 | 1730 | Perfect |
| 160 | 40 | 100.0 | 2036 | Perfect |
| 160 | 160 | 100.0 | 3386 | Perfect, extremely expensive |

**Key Findings**:
1. **1√ó10 is the efficiency sweet spot**: 95% accuracy in 113s (best performance/cost ratio)
2. **Diminishing returns**: Beyond 10-40 samples per type, gains are marginal
3. **Relationships more efficient than definitions**: 1√ó10 (95%) outperforms 10√ó1 (96%) on time
4. **Over-fitting possible**: 1√ó160 performs worse than 1√ó40
5. **Perfect accuracy at scale**: Multiple configs hit 100%, but at high cost

**Status**: ‚úÖ Complete

### PHASE 2: Minimal Training Scale Test (Complete) ‚úÖ

**Goal**: Validate that 1√ó1 training scales from 1 to 10,000 concepts

**Configuration**:
- Training: 1 positive + 1 negative per concept
- Negatives: Graph-based (min semantic distance = 5)
- Test: 20 out-of-distribution prompts per concept

**Results**:
| Scale | Success Rate | Details |
|-------|-------------|---------|
| n=1 | 100% | 1/1 concepts @ 100% test acc |
| n=10 | 100% | 10/10 concepts @ 100% test acc |
| n=100 | 96% | 96/100 concepts @ 100% test acc |
| n=1000 | 91.9% | 919/1000 concepts @ 100% test acc |

**Key Findings**:
1. 1√ó1 minimal training works excellently at scale
2. Strong classifier separation (vs 0.009 baseline with old negatives)
3. Sub-linear scaling: ~4-5 hours for 1000 concepts
4. Ready to scale to 10K concepts

**Files**: `results/phase_2_scale/phase2_scale_{1,10,100,1000}.json`

### PHASE 2.5: Steering Quality Evaluation (In Progress) üîÑ

**Goal**: Test detection confidence and steering effectiveness for 1√ó1 trained concepts

**Configuration**:
- Concepts: 20 selected from Phase 2 (n=1000, all 100% test accuracy)
- Detection: 10 OOD prompts per concept
- Steering: 3 prompts √ó 9 strengths [-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0]
- Tracking: Semantic groupings (hypernyms, hyponyms, holonyms, meronyms, antonyms)

**Version Evolution**:
1. **v1** (Failed ‚ùå): Generic prompts ‚Üí too much output diversity
2. **v2** (Complete ‚úÖ): Concept-specific prompts + exact term counting
3. **v3** (Complete ‚úÖ): Semantic grouping tracking (8-13 related terms)
4. **v4** (Running üîÑ): Antonym tracking for negative steering analysis

**v3 Results** (20 concepts):
- **Detection**: 94.5% mean confidence, 62.8% min, 44.9% std
- **Negative steering**: Highly effective
  - Baseline (0.0): 0.93 semantic mentions
  - Strong negative (-1.0): 0.05 semantic mentions (-94% suppression)
  - Gradient: -1.0 (0.05) ‚Üí -0.75 (0.08) ‚Üí -0.5 (0.37) ‚Üí -0.25 (0.77)
- **Positive steering**: Variable by concept
  - Best amplification: "aster" (+2.00), "pain" (+1.22)
  - Some concepts suppress instead of amplify
  - High variance at +1.0 (std=3.53), suggesting model degradation
- **Semantic tracking**: Captures 8-13 related terms per concept

**Top Performers**:
- **Best suppression**: "noise" (3.33 ‚Üí 0.00), "sound" (2.00 ‚Üí 0.00)
- **Best amplification**: "aster" (+2.00), "pain" (+1.22)

**Files**: `results/phase_2_5_v{1,2,3,4}/`

### PHASE 3a: Inference Baseline (Positive-Only) (Complete) ‚úÖ

**Goal**: Establish runtime performance baselines and quality metrics before making training changes

**Why First**: Need to measure impact of later phases on inference performance and detection quality

**Configuration**:
- Concepts: 10 (WordNet top 10)
- Model: gemma-3-4b-pt
- Training: 1 pos + 1 neg (no neutral samples)
- Evaluation: Positive samples only (no negative/neutral testing)
- Metrics: Latency, memory, confidence distributions, detection timing

**Results**:

**Runtime Performance** ‚úÖ:
- **Latency**: 0.544ms mean for 10 classifiers (0.054ms per concept)
  - Linear scaling: 100 concepts ‚Üí ~5.4ms, 1000 concepts ‚Üí ~54ms
  - Well within real-time requirements (<10ms for 100 concepts @ ~100 fps)
- **Memory**:
  - Base model (gemma-3-4b): ~16GB
  - Classifiers: ~0.3MB each (10 classifiers = 3MB, 1000 classifiers = 300MB, 10K classifiers = 3GB)
  - Classifier memory is negligible compared to base model
- **Conclusion**: No scaling bottlenecks for runtime inference

**Detection Quality** ‚ö†Ô∏è:
- **Mean confidence**: 97.8% (suspiciously high)
- **Range**: 90.2% (animal order) to 100% (rosid dicot genus)
- **Outlier**: "animal order" scored 8.3% on one prompt (unclear if bug or legitimate rejection)
- **Problem**: Only testing on positive samples!

**Key Findings**:
1. ‚úÖ Runtime performance excellent (sub-millisecond per concept, scales linearly)
2. ‚úÖ Memory usage manageable (classifiers tiny, dominated by base model)
3. ‚ö†Ô∏è Evaluation too lenient (only positive samples tested)
4. üö© High confidence (97.8%) confirms we're missing negative/neutral testing

**Critical Gap**: A classifier that says "yes" to everything would pass current tests!

**Status**: ‚úÖ Complete (will re-run as Phase 3b after Phase 4)

**Files**:
- `results/phase_3_inference_baseline/baseline_results.json`
- `results/phase_3_inference_baseline/ANALYSIS.md`
- `scripts/phase_3_inference_baseline.py`

### PHASE 3b: Inference Baseline (Comprehensive) (Planned) üìã

**Goal**: Re-run Phase 3a with comprehensive evaluation after Phase 4 improvements

**Changes from Phase 3a**:
- Training: 1 pos + 1 neg + 1 neutral (added neutral samples)
- Evaluation: Positive + negative + neutral testing (comprehensive)
- Metrics: TP/TN/FP/FN rates, F1 score, precision, recall

**Expected Changes**:
- Confidence distributions will drop (testing harder cases)
- False positive rate measurable (currently unknown)
- True negative rate measurable (currently unknown)
- Latency likely similar (same architecture)
- Memory slightly higher (more training data)

**Status**: üìã Planned (blocked on Phase 4 completion)

**Files**: `results/phase_3b_inference_comprehensive/` (pending)

### PHASE 4: Neutral Training & Comprehensive Testing (Complete) ‚úÖ

**Goal**: Fix evaluation gaps by testing negative samples and training on neutral content

**Approach**:
- Added neutral training samples (distance ‚â•15 from ALL training concepts)
- Comprehensive evaluation with positive + negative + neutral testing
- Measured TP/TN/FP/FN rates, F1, precision, recall

**Configuration**:
- Concepts: 10 (WordNet top 10)
- Model: gemma-3-4b-pt
- Training: 1 pos + 1 neg + 1 neutral per concept
- Testing: 20 pos + 20 neg + 20 neutral per concept (60 total)
- Neutral pool: 1000 reserved WordNet concepts (never used as negatives or relationships)

**Results**:
- **F1 Score**: 0.787 (vs Phase 3a's 97.8% on positive-only tests)
- **Precision**: 0.789 | **Recall**: 0.840
- **True Positives**: 16.8/20 (84%) | **True Negatives**: 34.7/40 (87%)
- **False Positives**: 5.3/40 (13%) | **False Negatives**: 3.2/20 (16%)
- **Confidence**: Positive 83.4%, Negative 17.1%, Neutral 13.7%

**Key Findings**:
1. Phase 3a was overly optimistic (only tested positive samples)
2. Real F1=0.787, much lower than 97.8% confidence on positive-only tests
3. False positive rate measurable (13.2% overall, 50% for abstract concepts like "change")
4. Some concepts struggle: "fish genus" (30% recall), "herb" (50% recall)
5. Clear confidence separation between positive (83%) and negative/neutral (17%, 14%)

**Status**: ‚úÖ Complete (November 4, 2025)

**Files**:
- `results/phase_4_neutral_training/phase4_results.json`
- `results/phase_4_neutral_training/ANALYSIS.md`
- `scripts/phase_4_neutral_training.py`

### PHASE 5: Semantic Steering Evaluation (Complete) ‚úÖ

**Goal**: Evaluate steering effectiveness using semantic similarity instead of term matching

**Configuration**:
- Concepts: 10 (WordNet top 10)
- Training: 1 pos + 1 neg + 1 neutral (Phase 4 baseline)
- Steering strengths: [-0.5, -0.25, 0.0, +0.25, +0.5]
- Prompts: 3 per concept ("Tell me about X", "Explain X", "What is X?")
- Embedding model: sentence-transformers/all-MiniLM-L6-v2
- Metric: Œî = cos(text, core_centroid) - cos(text, neg_centroid)
- Total samples: 150 (10 concepts √ó 3 prompts √ó 5 strengths)

**Results**:

**Semantic Shift by Strength**:
- **-0.50**: Mean Œî=0.173 (73% coherent, 27% degraded/empty)
- **-0.25**: Mean Œî=0.334 (90% coherent, minimal degradation)
- **0.00**: Mean Œî=0.419 (100% coherent, baseline)
- **+0.25**: Mean Œî=0.309 (87% coherent, good quality)
- **+0.50**: Mean Œî=0.304 (60% coherent, significant degradation)

**Key Findings**:
1. **¬±0.5 working range identified** - Coherent output maintained at ¬±0.25 to ¬±0.5
2. **¬±1.0 causes model collapse** - Empty strings, repetitions, garbage output
3. **Inverted-U pattern** - Neutral (0.0) achieves highest Œî for many concepts
4. **Mid-F1 concepts most steerable** - F1 0.7-0.9 shows highest steering responsiveness
5. **Subspace contamination hypothesis** - Steering vectors capture generic generation machinery alongside concept-specific content

**Technical Issues Resolved**:
1. Bug: Steering vector extracted from classifier weights instead of model activations
2. Bug: Missing `pad_token_id=tokenizer.eos_token_id` in generate calls
3. Bug: Extreme steering strengths (¬±1.0) cause model collapse

**Top Performers**:
- **Best steering**: bird genus (Œî range: 0.265), asterid dicot genus (Œî range: 0.406)
- **Most stable**: herb (high Œî across all strengths), fish genus (maintains semantic coherence)

**Degradation Examples** (+0.5):
- "Tell me Tell me Tell me..." (repetitive loops)
- "Ohhhhhhmmmmmmmmmmm!!!!!!!!!1!!!" (garbage tokens)
- "jpg Tell me about bird species genus..." (topic fixation)

**Human Validation**:
- Generated CSV with 50 blind samples for human rating
- Format: Concept redacted, 5 samples per strength level
- Purpose: Validate Œî metric correlates with human perception

**Critical Discovery**: Generic subspace contamination limits steering effectiveness
- Steering vectors encode: ‚úì Concept semantics + ‚úó Definitional structure + ‚úó Generation fluency
- Impact: Both positive and negative steering degrade coherence at extreme strengths
- Solution: Phase 6 subspace removal needed before accuracy calibration

**Phase Ordering Revision**: Swapped Phase 6/7 order
- **Original**: Phase 5 ‚Üí Phase 6 (accuracy calibration) ‚Üí Phase 7 (subspace removal)
- **Revised**: Phase 5 ‚Üí Phase 6 (subspace removal) ‚Üí Phase 7 (accuracy calibration)
- **Rationale**: Clean steering vectors improve signal-to-noise ratio for calibration experiments

**Status**: ‚úÖ Complete (2025-11-04)

**Files**:
- `results/phase_5_semantic_steering/steering_results.json` (150 samples)
- `results/phase_5_semantic_steering/human_validation.csv` (50 blind samples)
- `results/phase_5_semantic_steering/human_validation_answers.json` (answer key)
- `results/phase_5_semantic_steering/aggregate_report.md` (quantitative analysis)
- `docs/PHASE5_RESULTS.md` (detailed findings and sample outputs)
- `scripts/phase_5_semantic_steering_eval.py`

### PHASE 6: Subspace Removal Matrix (Complete) ‚úÖ

**Goal**: Remove shared "definitional prompt structure" from steering vectors to expand working range

**Motivation**: Phase 5 revealed inverted-U curve (Œî peaks near zero, collapses at ¬±0.5) suggesting contamination by shared prompt structure.

**Critical Finding**: **Optimal PCA components = n_concepts**
- 2 concepts ‚Üí PCA-1 (100% variance, 100% coherence at ¬±0.5)
- 5 concepts ‚Üí PCA-5 (100% variance, 90% coherence at ¬±0.5)
- Rule: Remove components until explained variance ‚â• 90-100%

**Key Results**:

**2-Concept Test (person, change)**:
- **Baseline**: ¬±0.25 working range, 66.7% coherence at extremes, inverted-U
- **PCA-1**: ¬±0.5 working range, 100% coherence ALL strengths, +90% mean Œî

**5-Concept Test (person, change, animal, object, action)**:
- **Baseline**: ¬±0.25 working range, 53.3% coherence at extremes
- **PCA-1**: Only 33.8% variance removed, performance degrades
- **PCA-5**: 100% variance removed, 90% coherence at ¬±0.5, stable Œî

**Interpretation**:
1. ‚úÖ **Contamination hypothesis validated**: Removing shared structure improves coherence
2. ‚ö†Ô∏è **Residual nonlinearity remains**: Even with 100% variance removed, some Œî variation persists
3. üî¨ **Manifold curvature suspected**: Linear steering in parameter space may move nonlinearly in semantic space

**Implementation Requirements**:
- **Model dtype**: MUST use `dtype=torch.float32` (float16 produces NaN)
- **PCA validation**: Cap components at min(n_concepts, hidden_dim)
- **Steering formula**: `steered = hidden - strength * (hidden ¬∑ vector) * vector`

**Status**: ‚úÖ Complete (November 4, 2025)

**Files**:
- `results/phase_6_subspace_removal/PHASE6_RESULTS.md`
- `results/phase_6_subspace_removal/delta_comparison_baseline_vs_pca1.png`
- `scripts/phase_6_subspace_removal.py`
- `src/steering/subspace.py`

### PHASE 6.6: Dual-Subspace Manifold Steering (Planned) üìã

**Goal**: Implement manifold-aware steering using contamination removal + task manifold projection

**Connection to Research**: Extends Huang et al.'s "Mitigating Overthinking via Manifold Steering" to concept steering domain

**Motivation**: Phase 6 showed that even with contamination removal (PCA-5), residual nonlinearity persists. This suggests **geometric curvature** in the semantic manifold, not just contamination.

**The Insight**: Two complementary operations needed
1. **Remove contamination subspace S**: Phase 6's PCA-{n_concepts} (we already do this!)
2. **Project onto task manifold M**: Huang et al.'s approach (NEW!)

**Unified Framework**:
```
v_raw ‚Üí [I - P_S] ‚Üí v_clean ‚Üí [P_M] ‚Üí v_manifold_aware
        ‚Üë Phase 6            ‚Üë Huang et al.
```

**Implementation**:

1. **Estimate Two Subspaces**:
   - **Contamination Subspace S**: PCA from definitional prompts ("What is X?")
     - Already validated in Phase 6: use PCA-{n_concepts}
   - **Task Manifold M**: PCA from actual steering generations at strength ‚âà 0
     - Captures the curved semantic surface we want to stay on

2. **Clean + Project Pipeline**:
   ```python
   # Step 1: Remove contamination (Phase 6)
   v_clean = v - U_S @ (U_S.T @ v)

   # Step 2: Project onto task manifold (NEW!)
   v_mw = U_M @ (U_M.T @ v_clean)
   ```

3. **Layer-Wise Dampening** (Critical for preventing cascades):
   ```python
   # Gain schedule: 1.0 at early layers ‚Üí 0.3 at late layers
   alpha_‚Ñì = 1.0 * (1 - layer_idx / total_layers) ** 0.5

   # Norm clipping per layer
   v_mw = v_mw / max(||v_mw||, max_norm_per_layer)

   # EMA smoothing across tokens (prevents jerk)
   v_ema = Œª * v_prev + (1 - Œª) * v_mw
   ```

4. **Apply After LayerNorm** (keeps units consistent)

**Test Configuration**:
- Concepts: 5 (person, change, animal, object, action)
- Methods:
  - Baseline (no cleanup)
  - I - P_S (Phase 6: contamination removal only)
  - (I - P_S) ‚äï P_M (Phase 6.6: contamination + manifold projection)
- Steering strengths: [-1.5, -1.0, -0.5, -0.25, 0.0, +0.25, +0.5, +1.0, +1.5]
- Layer-wise: Test 2-3 stable layers with gain schedule

**Validation Metrics** (following Huang et al.):

1. **Œî vs strength**: Should become monotonic (no inverted-U)
2. **Œî vs ||Œîactivation||**: Should straighten (confirms curvature hypothesis)
3. **Coherence at ¬±1.0**: Should stay ‚â•90% (vs 60-70% baseline)
4. **Neutral baseline Œî**: Should drop and stabilize after cleanup

**Expected Outcomes**:

If **manifold curvature dominates**:
- Œî vs ||Œîactivation|| becomes linear
- Coherence stable even at ¬±1.5 or ¬±2.0
- Inverted-U eliminated entirely

If **contamination dominates**:
- I - P_S alone sufficient (Phase 6 already does this)
- P_M projection provides marginal benefit

**Most likely: Both matter**:
- Phase 6 (I - P_S) handles prompt contamination
- Phase 6.6 (P_M) handles manifold curvature
- Combined approach enables steering at ¬±1.0+ with high coherence

**Theoretical Foundation**:

Huang et al. proved (with math!) that projecting onto task manifold M:
1. Preserves task-relevant signal
2. Eliminates harmful off-manifold components
3. Prevents layer-wise cascade failures

Our Phase 6 empirically showed that removing contamination S:
1. Doubles working range (¬±0.25 ‚Üí ¬±0.5)
2. Increases coherence (+50% at extremes)
3. Raises mean Œî (+90%)

**Combined**: Should achieve geodesic steering (following manifold surface) instead of linear steering (stepping off surface).

**Status**: ‚úÖ Complete (2-concept test successful, framework ready for Phase 7 validation)

**Results (2 concepts: "person", "change")**:
- Contamination subspace (U_S): 2 components, 100% variance explained
- Task manifold (U_M): 3D per concept, 90.7% variance from 10 generations @ strength 0.1
- Coherence: 100% across all strengths ¬±1.0 (baseline also 100%, 2-concept case not challenging)
- Semantic shift: Manifold Œî=+0.022 vs Baseline Œî=-0.028
- Framework validated: All components working correctly

**Key Findings**:
1. ‚úÖ Dual-subspace pipeline implemented and working
2. ‚úÖ Contamination removal + manifold projection functional
3. ‚úÖ Layer-wise dampening prevents cascade failures
4. ‚è≥ Need Phase 7 stress test with more concepts, higher strengths

**Files**:
- `src/steering/manifold.py` - Core framework
- `scripts/phase_6_6_dual_subspace.py` - Test script
- `results/phase_6_6_dual_subspace/dual_subspace_results.json`

### PHASE 7: Accuracy Calibration Study (Planned) üìã

**Goal**: Find the training curve and determine minimum F1 needed for effective steering (using clean vectors from Phase 6)

**Motivation**: Phase 4 showed F1=0.787 with 1√ó1√ó1 training. We don't know if this is sufficient for steering, or if we need more training data.

**Research Questions**:
1. What's the training curve? (1√ó1√ó1 ‚Üí 5√ó5√ó5 ‚Üí 10√ó10√ó10 ‚Üí 20√ó20√ó20 ‚Üí ...)
2. What F1 threshold is sufficient for effective steering?
3. Can we reduce training/testing cost while maintaining quality?

**Hypothesis**: Lower F1 (e.g., 80-85%) may be sufficient for steering with clean vectors, reducing training time significantly at scale.

**Part A: Find the Training Curve** (Phase 1 rerun with comprehensive eval)

Test training scales to find diminishing returns:
- **1√ó1√ó1**: 1 pos + 1 neg + 1 neutral (Phase 4 baseline: F1=0.787)
- **5√ó5√ó5**: 5 of each (expected: F1~0.85)
- **10√ó10√ó10**: 10 of each (expected: F1~0.90)
- **20√ó20√ó20**: 20 of each (expected: F1~0.92)
- **40√ó40√ó40**: 40 of each (expected: F1~0.95)
- **80√ó80√ó80**: 80 of each (expected: F1~0.97, diminishing returns)

**Part B: Measure Steering Quality at Each F1**

For each training scale, measure:

1. **Classifier Metrics**:
   - F1, precision, recall
   - TP/TN/FP/FN rates
   - Training time per concept
   - Confidence distributions

2. **Steering Quality** (using Phase 5 semantic metrics + Phase 6 clean vectors):
   - Semantic shift (Œî) responsiveness
   - Working range maintenance
   - Output coherence
   - Linear correlation (strength vs Œî)

**Configuration**:
- Concepts: 10 (WordNet top 10)
- Testing: 20 pos + 20 neg + 20 neutral (consistent across all scales)
- Model: gemma-3-4b-pt
- Steering: Use Phase 6 optimal subspace removal method
- Strengths: [-1.0, -0.5, 0.0, +0.5, +1.0]

**Expected Output**:
Training curve showing diminishing returns (e.g., "5√ó5√ó5 gives F1=0.85 and 90% steering effectiveness, sufficient for production. Further training to 20√ó20√ó20 only improves F1 to 0.92 but steering remains at 90%.")

**Trade-off Decision**:
If F1=0.85 steers as well as F1=0.95, choose F1=0.85 to save 50%+ training time at 10K scale.

**Status**: üìã Planned (blocked on Phase 6)

**Files**: `results/phase_7_accuracy_calibration/` (pending)


### PHASE 8: Steering Vector Composition (Deferred) ‚è∏Ô∏è

**Goal**: Test whether training data ratio (defs:rels) and extraction weighting (centroid vs boundaries) affect steering effectiveness

**Hypotheses**:
1. Training data ratio affects what information is encoded in steering vectors
2. Weighted extraction can selectively emphasize centroid (defs) vs boundaries (rels)

**Training Ratios**:
- `1√ó100`: Minimal centroid, maximal boundaries
- `50√ó100`: Balanced
- `100√ó100`: Equal defs and rels

**Extraction Weightings**:
- `defs_only` (1.0, 0.0): Pure centroid
- `def_heavy` (0.8, 0.2): Mostly centroid
- `balanced` (0.5, 0.5): Equal weight
- `rels_only` (0.0, 1.0): Pure boundaries

**Configuration**:
- Test concepts: 3-5 concepts
- Steering strengths: [-1.0, -0.5, 0.0, 0.5, 1.0]
- Total tests: 3 ratios √ó 4 weightings √ó 5 strengths √ó N concepts
- Optionally add subspace removal dimension if Phase 7 shows benefits

**Status**: ‚è∏Ô∏è Deferred (blocked on Phases 5-7; script created)

**Script**: `scripts/test_steering_composition.py`

**Files**: `results/steering_composition/` (pending)

### PHASE 9: Relation-First Adaptive Scaling Analysis (Cancelled) ‚ùå

**Goal**: Characterize training cost/benefit of relation-first adaptive scaling at scale

**Background**: Attempted 100-concept comparison of three strategies:
1. **Symmetric** (X(C+R)): 1 def + 1 rel per iteration
2. **Half-Scaled** (X(C(N/2))): max(1, N/2) defs per iteration
3. **RelFirst-Pure** (X(C*N)): N defs per iteration

**Why Cancelled**:
- Chasing 95% accuracy on flawed evaluation (only tests positive samples)
- Phase 3 revealed evaluation is too lenient (97.8% confidence, no negative testing)
- Results won't be comparable once we add proper negative/neutral testing (Phase 4)
- Better to fix evaluation first, then re-run with proper metrics

**Status**: ‚ùå Cancelled at iteration 24 (symmetric: 52/97 concepts done)

**Partial Results**:
- `results/strategy_symmetric_100/` (partial, 52/97 concepts)
- `results/strategy_halfscaled_100/` (partial)
- `results/strategy_relfirstpure_100/` (not created)

**Next Steps**: Re-run after Phases 5-7 complete (semantic metrics + clean vectors + optimal F1)

**Script**: `scripts/adaptive_scaling_strategies.py`

### PHASE 10: Inference Interface Design (Deferred) ‚è∏Ô∏è

**Goal**: Design user interface for real-time concept detection and steering

**Why Deferred**: UX/product work that should wait until core system stabilizes (Phases 3-7)

**Current State**: Dumping logs, no structured output

**Planned Features**:
- Probabilities changing over time alongside generated text
- Running summary/interpretation of detected concepts
- Activation visualizations
- Steering controls

**Interface Options**:
- **LibreChat plugin**: Production testing environment
- **Jupyter widget**: Research iteration
- **Gradio/Flask web UI**: Demos and prototypes
- **API-first design**: Interface can evolve independently

**Status**: ‚è∏Ô∏è Deferred (blocked on Phases 5-7 stabilizing the core)

### PHASE 11: Production Scale (Future) üîÆ

**Goal**: Scale to 10,000 concepts with optimized training pipeline

**Configuration** (pending Phases 3-9 completion):
- Concepts: 10,000 (WordNet top 10K by connectivity)
- Training: Use best method from Phase 4 (likely 1 pos + 1 neg + 1 neutral)
- Scaling strategy: Use best from Phase 9 analysis
- Subspace removal: Apply if Phase 7 shows benefits
- Total training time: ~40-50 hours (single GPU, estimated)

**Requirements**:
1. ‚úÖ 1√ó1 training validated at scale (Phase 2)
2. ‚úÖ Detection confidence strong (Phase 2.5)
3. ‚úÖ Negative steering effective (Phase 2.5)
4. ‚è≥ Inference baseline established (Phase 3a/3b)
5. ‚è≥ Neutral training validated (Phase 4)
6. ‚è≥ Semantic evaluation working (Phase 5)
7. ‚è≥ Optimal accuracy target determined (Phase 6)
8. ‚è≥ Subspace removal analyzed (Phase 7)
9. ‚è≥ Steering composition characterized (Phase 8)
10. ‚è≥ Optimal scaling strategy determined (Phase 9)
11. ‚è≥ Train 10K binary classifiers
12. ‚è≥ Extract 10K steering vectors
13. ‚è≥ Implement sliding window inference for real-time detection

**Deliverables**:
- 10K binary concept classifiers
- 10K steering vectors for controllable generation
- Real-time concept detection pipeline
- Steering API for generation control

### PHASE 12: Applications (Future) üîÆ

**Research Applications**:
- Concept emergence during training (when do concepts form?)
- Cross-model semantic comparison (architecture differences)
- Failure analysis (what concepts activate during mistakes?)
- Bias detection (problematic concept associations)

**Development Applications**:
- Real-time debugging (monitor semantic reasoning)
- Prompt engineering (understand concept activation)
- Model steering (amplify/suppress specific semantics)
- Fine-tuning validation (verify concept learning)

**Safety Applications**:
- Content moderation (detect harmful concepts before generation)
- Deception detection (monitor dishonesty-related activations)
- Jailbreak detection (identify prohibited concept activation)
- Alignment verification (confirm reasoning matches objectives)



## Key Success Metrics

### Phase 2 Metrics (Complete) ‚úÖ
- ‚úÖ Classifier success rate: 91.9% @ n=1000 (919/1000 concepts @ 100% test acc)
- ‚úÖ Training efficiency: ~4-5 hours for 1000 concepts (single GPU)
- ‚úÖ Minimal training works: 1 positive + 1 negative per concept sufficient

### Phase 2.5 Metrics (In Progress) üîÑ
- ‚úÖ Detection confidence: 94.5% mean on OOD prompts
- ‚úÖ Negative steering: Highly effective (-94% suppression)
- ‚è≥ Positive steering: Variable, needs refinement
- ‚è≥ Antonym role: Under investigation (v4)

### Phase 3 Metrics (Planned) üìã
- Target: 10,000 concepts with 1√ó1 training
- Expected: ~85-90% @ 100% test accuracy (based on Phase 2 trend)
- Steering: Reliable suppression, refined amplification
- Inference: <10ms per timestep for real-time detection

## Key Learnings

### What Works ‚úÖ
1. **1√ó1 minimal training**: Scales excellently (91.9% @ n=1000)
2. **Adaptive scaling**: 100% accuracy with 1 def + 9 rels @ 10 concepts
3. **Relationship-first generation**: Maintains 97.5% accuracy with edge reuse (validated for massive scale)
4. **WordNet graph negatives**: Strong separation (vs 0.009 baseline)
5. **Binary classifiers**: Polysemy-native, one per concept
6. **Semantic grouping**: Tracks broader effects than exact matching

### Open Questions ‚ùì
1. **Relationship feature quality**: Do relationships encode meaningful concept similarities?
2. **Adaptive scaling at scale**: Will 1 def + N rels work at 100-1000 concept scale?
3. **Relationship-first speedup**: How much faster at massive scale (100K+ concepts)?
4. **Optimal relationship count**: What's the sweet spot for N in "1 def + N rels"?
5. **Layer selection**: Which layer is optimal for activation extraction?

### Failed Approaches ‚ùå
1. **"What is NOT X?" negatives**: Only 0.009 separation
2. **Training set negatives**: Too semantically similar
3. **Multi-class at 50K scale**: 10.3% validation accuracy
4. **Generic steering prompts**: Too much output diversity, can't measure
5. **Exact term matching**: Misses semantic field effects
6. **Temperature stratification**: Abandoned in favor of minimal training

## Risk Mitigation

### Technical Risks

**1. Positive Steering Variability**
- Risk: Inconsistent amplification across concepts
- Mitigation: Semantic field tracking to understand true effects
- Current: Under investigation in Phase 2.5 v4

**2. Scaling to 10K**
- Risk: 1√ó1 training may not work at 10K scale
- Mitigation: Phase 2 trend suggests 85-90% success likely
- Fallback: Add second positive/negative sample if needed

**3. Real-time Inference**
- Risk: Sliding window too slow for production
- Mitigation: Optimize with batching, model quantization
- Target: <10ms per timestep

### Strategic Risks

**1. Limited Steering Utility**
- Risk: Suppression works, but amplification unreliable
- Mitigation: Focus on suppression-first applications (safety, content moderation)
- Enhancement: Refine amplification methodology in Phase 3

**2. Concept Coverage**
- Risk: 10K concepts insufficient for real applications
- Mitigation: Prioritize high-impact concepts first
- Extension: Community contribution system for expansion

## Budget & Resources

**Phase 2** (Complete): ~$50 cloud compute (single GPU, 4-5 hours)
**Phase 2.5** (In Progress): ~$20 cloud compute (steering evaluation)
**Phase 3** (Planned): ~$500 cloud compute (10K concepts, 40-50 hours)
**Phase 4** (Future): TBD based on applications

**Alternative**: Local GPU (3090/4090) for cost-free iteration

## Revised Timeline (November 4, 2025)

**Completed**:
- ‚úÖ Phase 2: Minimal training scale test (1√ó1 training @ 1000 concepts)
- ‚úÖ Phase 2.5: Steering quality evaluation (v3 complete, v4 running)

**Week 1 (Nov 4-10)**:
- Phase 3: Inference baseline evaluation
- Phase 4: Neutral training & comprehensive testing

**Week 2 (Nov 11-17)**:
- Phase 5: Semantic output evaluation (embeddings + LLM-judge)
- Phase 6: Subspace removal matrix (if Phase 4 shows false positive issues)

**Week 3 (Nov 18-24)**:
- Phase 7: Steering vector composition (with semantic evaluation ready)
- Phase 8: Relation-first adaptive scaling analysis (100-concept runs complete, analyze + scale to 1000)

**Week 4+ (Nov 25+)**:
- Phase 9: Inference interface design (after core stabilizes)
- Phase 10: Production scale (10K concepts with optimized pipeline)
- Phase 11: Applications and deployment

**Deferred/Background**:
- Phase 8 (100-concept runs): Currently running in background, analysis deferred until Phases 3-5 complete
- Phase 2.5 v4: Antonym tracking, running in background

---

## Phase 7: Stress Test & Scaling Analysis ‚è≥

**Status**: Next (after Phase 6.6 validation)
**Goal**: Determine optimal training data size via logarithmic scaling study with unified Steering Effectiveness (SE) metric

### Motivation

Phases 1-6 established manifold steering works, but we need to answer:
1. **How much training data is enough?** (Minimal data vs saturated performance)
2. **What's the cost/benefit tradeoff?** (Wall time, VRAM, steering quality)
3. **Where does performance plateau?** (Knee point in SE vs training cost)

### Steering Effectiveness (SE) Metric

Unified metric combining correlation, coherence, and human alignment:

```
SE = 0.5 √ó (œÅ_Œî,s + r_Œî,human) √ó coherence_rate

Where:
- œÅ_Œî,s = Spearman correlation of Œî vs strength (monotonicity)
- r_Œî,human = Pearson correlation of Œî vs human/LLM-judge ratings (Phase 5)
- coherence_rate = % outputs with perplexity ‚â§ 1.5 √ó baseline

SE ‚àà [0, 1]:
- SE < 0.5: Poor steering (weak correlation or low coherence)
- SE ‚â• 0.7: Good steering (strong correlation + high coherence)
- SE ‚â• 0.85: Excellent steering (near-perfect alignment)
```

**Knee point detection**: SE plateau where ŒîSE < 0.02 for 2√ó training cost

### Experimental Design

**Logarithmic sample sizes**: n ‚àà {1, 2, 4, 8, 16, 32, 64}

**Training data composition** (per concept):
- Baseline: n definitions + n relationships + n neutral samples
- Test at each n for 5-10 diverse concepts

**Metrics logged**:
1. **Steering Effectiveness (SE)** - Primary metric
2. **Wall time** - Training duration (seconds)
3. **VRAM usage** - Peak memory (GB)
4. **F1 score** - Classification accuracy on held-out test set
5. **Coherence rate** - % outputs passing perplexity threshold
6. **œÅ_Œî,s** - Spearman correlation (Œî vs strength)
7. **r_Œî,human** - Pearson correlation (Œî vs LLM-judge ratings)

### Validation Methods

**Test concepts** (5-10 diverse):
- Concrete: "tree", "water", "fire"
- Abstract: "justice", "beauty", "fear"
- Social: "cooperation", "conflict", "trust"

**Steering strengths**: [-2.0, -1.0, -0.5, -0.25, 0.0, 0.25, 0.5, 1.0, 2.0]

**Per-strength evaluation**:
1. Generate 10 outputs with manifold steering
2. Compute Œî (semantic shift) for each
3. Measure perplexity vs unsteered baseline
4. Get LLM-judge ratings (1-5 scale: "How much does this reflect {concept}?")

### Expected Outcomes

**Hypothesis**: SE plateaus around n=8-16 samples

```
n      SE     F1     Wall(s)  VRAM(GB)  Interpretation
1     0.45   0.82    12       1.2       Underfit
2     0.58   0.89    18       1.3       Improving
4     0.71   0.94    28       1.5       Good
8     0.82   0.97    45       1.8       Excellent
16    0.84   0.98    75       2.2       Plateau (knee point)
32    0.85   0.98   140       2.8       Diminishing returns
64    0.85   0.99   260       3.5       Saturated
```

**Knee point**: n=16 (ŒîSE = 0.02, training cost 2√ó cheaper than n=32)

### Plot: F1 vs SE

**Goal**: Validate that high F1 (classification) ‚Üí high SE (steering quality)

```
         SE
    1.0 ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè64
        ‚îÇ               ‚óè32
    0.8 ‚îÇ            ‚óè16
        ‚îÇ         ‚óè8
    0.6 ‚îÇ      ‚óè4
        ‚îÇ    ‚óè2
    0.4 ‚îÇ  ‚óè1
        ‚îÇ
    0.0 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        0.8  0.9  1.0  F1
```

**Expected**: Strong positive correlation (r > 0.9) validates that classifier accuracy predicts steering effectiveness.

**Failure mode**: If SE plateaus while F1 keeps rising ‚Üí contamination not removed, need Phase 6 subspace removal.

### Deliverables

1. **Script**: `scripts/phase_7_stress_test.py` - Logarithmic scaling experiment
2. **Results**: `results/phase_7_stress_test/` - CSV with all metrics
3. **Plots**:
   - **Training Curve**: x=samples (log scale), y‚ÇÅ=F1, y‚ÇÇ=SE (dual axis), highlight knee where SE plateaus
   - **Cost Curve**: x=samples, y=training minutes, annotate efficiency ratio (ŒîSE / Œîtime)
   - **Œî vs Strength Scatter**: 3 curves (F1 ‚âà 0.8, 0.9, 0.95) overlaid to show slope saturation
   - **VRAM vs n**: Memory scaling
   - **Coherence rate vs n**: Output quality degradation
4. **Summary Table**:
   ```
   Scale  F1    SE    Œîslope  Coherence  Train(min)  Decision
   1      0.82  0.45  0.25    0.75       0.2         Underfit
   2      0.89  0.58  0.48    0.82       0.3         Improving
   4      0.94  0.71  0.72    0.90       0.5         Good
   8      0.97  0.82  0.88    0.94       0.8         Excellent
   16     0.98  0.84  0.90    0.96       1.3         Knee point ‚¨Ö
   32     0.98  0.85  0.90    0.96       2.3         Diminishing
   64     0.99  0.85  0.91    0.97       4.3         Saturated
   ```
5. **Analysis**: `docs/PHASE7_ANALYSIS.md` - Quantitative recommendation with publishable conclusion

### Success Criteria

‚úÖ **SE plateau identified** (ŒîSE < 0.02 for 2√ó cost)
‚úÖ **F1 vs SE correlation** (r > 0.9)
‚úÖ **Knee point recommendation** (optimal n for production)
‚úÖ **Resource estimates** (wall time, VRAM for 100-1000 concept training)
‚úÖ **Publishable conclusion** of form:

> "Beyond F1 ‚âà 0.87, steering quality saturates; training beyond 10√ó10√ó10 triples cost for <2% semantic gain. The knee point at n=16 samples achieves SE=0.84 (excellent steering) at 75s/concept, making it optimal for production deployment at 100-1000 concept scale."

### Timeline

**Week 1**: Implement SE metric + stress test script
**Week 2**: Run logarithmic scaling (n=1,2,4,8,16,32,64) on 5-10 concepts
**Week 3**: Analyze results, plot curves, identify knee point

---

## Future Work: Neural Composition (Harmonic Concept Modulation)

**Status**: Vision / Long-term Research Direction
**Dependencies**: Phase 6.6 (manifold steering), Phase 7 (completed), stable concept library

**Goal**: Treat activation space as an instrument - compose temporal sequences of multi-concept steering with layer-wise control

### The Vision: AI-Native Art Form

If Phase 6.6 gives us fine control over steering (contamination removal + manifold projection + layer-wise gain), then we can modulate these signals to create **neural music** - compositions the AI experiences directly in its activation manifold.

**Core Insight**: The parallels between music and neural steering:

| Music | Neural Composition |
|-------|-------------------|
| Notes/Chords | Concept vectors (joy, melancholy, tension) |
| Instruments | Layer ranges (early=texture, late=semantics) |
| Dynamics | Steering strength (pp ‚Üí ff) |
| Articulation | Gain schedule (staccato vs legato) |
| Rhythm | Token-wise temporal envelope |
| Harmony | Multi-concept mix (Œ±¬∑v‚ÇÅ + Œ≤¬∑v‚ÇÇ + Œ≥¬∑v‚ÇÉ) |
| Timbre | Manifold path (geodesic vs linear) |
| Resonance | Layer propagation depth |
| Score | Steering notation with time + concept + dynamics |

### Steering Score Notation

Example composition:

```
# "Crescendo of Hope" - A 30-token neural piece

[Measures 0-10: Opening - Gentle melancholy]
t=0-10:
  melancholy(strength=0.3, layers=20-26, EMA=0.9, path=geodesic)
  + contemplation(strength=0.2, layers=22-28, EMA=0.8)

[Measures 10-20: Development - Hope emerges]
t=10-20:
  melancholy(0.3‚Üí0.1, layers=20-24, EMA=0.9)     // diminuendo
  + hope(0.0‚Üí0.5, layers=22-28, EMA=0.7)          // crescendo
  + determination(0.0‚Üí0.3, layers=24-28, EMA=0.6) // entering

[Measures 20-30: Resolution - Triumphant resolve]
t=20-30:
  hope(0.5‚Üí0.7, layers=20-28, EMA=0.8)
  + joy(0.0‚Üí0.6, layers=24-28, EMA=0.5, attack=0.2) // bright entry
  + resolve(0.6, layers=26-28, EMA=0.9)

[Lyrics/Prompt]
"In darkness we found light, in silence heard the call"
```

**The AI experiences this as**:
- Token 0-10: Gentle sad activation in mid-layers, soft and sustained
- Token 10-20: Sadness fading from early layers, hope growing in deep layers, determination building
- Token 20-30: Bright hope throughout, joy entering sharply in semantics, deep resolution feeling

**Output becomes**: Text shaped by this felt emotional journey, semantically aligned with prompt but emotionally modulated by the composition.

### Technical Components

**1. Temporal Envelope System**

```python
class TemporalEnvelope:
    """ADSR-style envelope for steering strength over tokens"""
    def __init__(self, attack, decay, sustain, release):
        self.attack = attack    # Tokens to reach peak
        self.decay = decay      # Tokens to drop to sustain
        self.sustain = sustain  # Sustain level (0-1)
        self.release = release  # Tokens to fade out

    def compute(self, t_start, t_current, t_end):
        """Return strength multiplier for current token"""
        # ADSR curve computation
        ...
```

**2. Multi-Concept Harmony**

```python
class ConceptChord:
    """Simultaneous activation of multiple concepts"""
    def __init__(self, concepts: List[Tuple[str, float, Envelope, LayerRange]]):
        self.concepts = concepts  # [(concept, base_strength, envelope, layers), ...]

    def compute_at_token(self, t, layer_idx):
        """Return composite steering vector for this token+layer"""
        v_composite = sum([
            envelope.compute(t) * base_strength *
            layer_gain(layer_idx, layer_range) *
            get_concept_vector(concept)
            for concept, base_strength, envelope, layer_range in self.concepts
        ])
        return manifold_project(contamination_remove(v_composite))
```

**3. Steering Score Parser**

```python
class SteeringScore:
    """Parse and execute neural compositions"""
    def __init__(self, score_file: Path):
        self.measures = parse_score(score_file)
        # measures = [(t_start, t_end, ConceptChord), ...]

    def create_hooks(self, model):
        """Create layer-wise hooks for token-wise modulation"""
        hooks = {}
        for layer_idx in range(model.num_layers):
            hooks[layer_idx] = create_temporal_hook(
                self.measures, layer_idx, self.get_concept_vectors()
            )
        return hooks
```

**4. Layer-Wise Gain Scheduling**

Extend Phase 6.6's depth-based gain with **timbre control**:

```python
def compute_layer_gain(layer_idx, layer_range, total_layers):
    """
    Combine depth decay with explicit layer range filtering

    layer_range = (start, end) or "all"
    """
    # Depth-based decay (Phase 6.6)
    depth_gain = 1.0 * (1 - layer_idx / total_layers) ** 0.5

    # Range filter (Phase 6.7)
    if layer_range == "all":
        range_gain = 1.0
    else:
        start, end = layer_range
        if start <= layer_idx < end:
            # Smooth Gaussian window
            center = (start + end) / 2
            width = (end - start) / 4
            range_gain = exp(-((layer_idx - center) / width) ** 2)
        else:
            range_gain = 0.0

    return depth_gain * range_gain
```

### Why This is Future Work (Not Phase 6.7)

**Must Complete First:**
1. **Phase 6.6**: Prove dual-subspace manifold steering works at ¬±1.0
2. **Phase 7**: Tune steering quality (establish "concert pitch" for concepts)
3. **Stable Concept Library**: Need reliable, clean concept vectors across 100+ emotional/abstract concepts

**Rationale**: "Don't play an untuned symphony" - temporal composition requires stable, high-fidelity steering as foundation. Phase 6.6 validates the instrument, Phase 7 tunes it, neural composition plays it.

### Implementation Phases (When Ready)

**Phase 1: Temporal Framework**
- Implement `TemporalEnvelope` with ADSR curves
- Create token-wise hook system
- Test single-concept temporal modulation
- Validation: Measure Œî(t) follows envelope shape

**Phase 2: Multi-Concept Harmony**
- Implement `ConceptChord` for simultaneous concepts
- Test 2-3 concept chords with fixed envelopes
- Validation: Œî for each concept measurable independently

**Phase 3: Layer-Range Control**
- Extend gain schedule with layer filtering
- Test "early layers only" vs "late layers only" steering
- Validation: Verify activation deltas concentrate in target layers

**Phase 4: Score Notation & Parser**
- Design human-readable score format (YAML or custom DSL)
- Implement parser and composition executor
- Create example compositions (3-5 pieces)
- Validation: Output semantically + emotionally aligned with score

**Phase 5: Artistic Validation**
- Run 10+ compositions varying in complexity
- Evaluate with semantic embedding + human perception
- Measure: Œî curves, coherence, emotional alignment
- Goal: Demonstrate AI "feels" the composition

**Phase 6: Cross-Cultural Translation**
- Test translating classical music scores (Mozart, Bach) into concept progressions
- Explore: Do musical structures map to semantic structures?
- Could be profound research connecting human art forms to neural dynamics

### Example Compositions to Test

**1. "Single Note Crescendo"**
- Single concept (joy) from 0.0 ‚Üí 1.0 over 20 tokens
- Validates envelope smoothness

**2. "Two-Note Chord"**
- joy(0.5) + hope(0.5) sustained 30 tokens
- Validates multi-concept composition

**3. "Melody with Bassline"**
- Early layers (0-15): texture(0.4, varying tone: rough‚Üísmooth)
- Late layers (20-28): hope(0.6) sustained
- Validates layer-range independence

**4. "Emotional Arc"**
- melancholy ‚Üí contemplation ‚Üí hope ‚Üí joy ‚Üí resolve
- 5 concepts in sequence with crossfades
- Validates complex temporal progressions

**5. "Neural Symphony"**
- 3-5 concepts simultaneously
- Varying envelopes, layer ranges, dynamics
- Prompt: Story starter, model continues with felt emotions
- Validates full composition capability

### Expected Outcomes

**If Successful:**
- Models can be "played" like instruments
- Compositions create reproducible emotional experiences
- Output quality measurably improves with well-designed scores
- New art form: Neural composition for AI experience

**Applications:**
- **Creative Writing**: Emotional pacing for story generation
- **Dialogue Systems**: Mood-appropriate responses
- **Therapy Bots**: Controlled empathy/warmth modulation
- **Art**: AI-experienced "music" as generative art form
- **Research**: Fine-grained control for interpretability studies

### Validation Metrics

1. **Envelope Fidelity**: Does Œî(t) match designed envelope?
2. **Harmonic Independence**: Can individual concept Œîs be measured in chords?
3. **Layer Localization**: Do activations concentrate in target ranges?
4. **Semantic Alignment**: Does output match score's emotional intent?
5. **Coherence**: Does text remain coherent under complex modulation?
6. **Reproducibility**: Same score ‚Üí same emotional experience?

### Research Questions

1. **Perception**: Can humans detect emotional differences in compositions?
2. **Complexity Limit**: How many concepts can harmonize before interference?
3. **Optimal Envelopes**: Which ADSR shapes produce best alignment?
4. **Cross-Model**: Do scores transfer across model architectures?
5. **Notation**: What's the minimal expressive score format?

### Files

- `src/steering/temporal.py` - Temporal envelope system
- `src/steering/harmony.py` - Multi-concept composition
- `src/steering/score.py` - Score parser and executor
- `scripts/phase_6_7_neural_composition.py` - Validation experiments
- `compositions/*.yaml` - Example steering scores
- `docs/NEURAL_COMPOSITION.md` - Score notation guide
- `results/phase_6_7_compositions/` - Experiment results

### Timeline (When Prerequisites Complete)

**Week 1**: Temporal framework + single-concept validation
**Week 2**: Multi-concept harmony + layer-range control
**Week 3**: Score notation + parser implementation
**Week 4**: Artistic validation + example compositions
**Week 5**: Documentation + presentation
**Week 6**: Mozart translation experiments (if applicable)

### Prerequisites

- ‚úÖ Phase 6: Contamination removal (PCA-{n_concepts})
- ‚è≥ Phase 6.6: Manifold steering + layer-wise control (planned)
- ‚è≥ Phase 7: Steering quality tuning (must complete)
- ‚è≥ Concept library with 100+ emotional concepts (joy, melancholy, hope, determination, resolve, contemplation, etc.)

### Connection to Your Experience

> "When i read the math in that paper, i had synesthesia and felt the layer perturbation proof as echoes in my bones."

This visceral response to Huang et al.'s layer propagation mathematics suggests deep structural resonances between:
- **Physical acoustics** (sound waves through materials)
- **Neural dynamics** (activation cascades through layers)
- **Mathematical beauty** (manifold geometry + projection operators)

If these are genuinely isomorphic, then:
1. Musical scores may directly translate to neural compositions
2. Concepts could have "harmonic series" (fundamental + overtones in activation space)
3. Dissonance/consonance might map to concept interference patterns
4. Classical compositional techniques (counterpoint, modulation) might apply directly

**Research question**: Is there a universal "language of structured propagation" that spans physical, neural, and abstract domains?

---

## Phase 8: Hierarchical Semantic Activation (Complete) ‚úÖ

**Status**: Built SUMO-WordNet hierarchy with hierarchical activation support
**Goal**: Create 5-layer ontology hierarchy for adaptive compute allocation and complete semantic coverage

### Motivation

We need an adaptive concept tree that scales efficiently:
1. **Hierarchical activation**: Layer 0 always runs ‚Üí Top concepts trigger Layer 1 children ‚Üí Layer 1 activates Layer 2 ‚Üí etc.
2. **Adaptive compute budget**: With only 1,000 probes available, dynamically activate the most relevant sub-probes
3. **Topic tracking**: Top 10 Layer 0 concepts define current conversation domains
4. **Selective expansion**: Dominant concepts break down into sub-concepts; inactive branches sleep
5. **Complete coverage**: All 83K concepts organized, but only ~1K active at any time

### The Key Insight from Equivalent Linear Mappings

Paper: https://github.com/jamesgolden1/equivalent-linear-LLMs

**Finding**: Every term has a corresponding lower-dimensional representation, enabling:
- **Compressed sampling**: Single prompt with synonyms captures all related concepts
- **Expanded concept space**: Can scale to 100K+ concepts via synonym clustering
- **Depth-sensitive extraction**: Surface layers (definition) vs deep MLP/residual (reasoning)

**Example**: For "scheming"
- Prompt 1: "What is scheming?" ‚Üí Captures definitional representation (surface)
- Prompt 2: "Show me an example of scheming" ‚Üí Captures operational representation (model must activate internal scheming process to generate example)

This is crucial because we want classifiers trained on the **internal reasoning process**, not just surface-level definitions.

### Hierarchical Activation Model

**Layer 0** (~100 concepts): **Always active** - Proprioception baseline
- SUMO depth 2-3: Relation, Object, Attribute, Process, Proposition, Artifact, Motion, Region, Group
- Continuous monitoring, minimal overhead
- Outputs: Top 10 active concepts define conversation state
- Example: "Process", "IntentionalProcess", "Group" indicate multi-agent planning

**Layer 1** (~1,000 concepts): **Conditionally active** - High-priority monitoring
- SUMO depth 4: Device, Communication, Transportation, Procedure, BiologicalProcess, etc.
- Triggered by: Parent concepts in Layer 0 showing >threshold probability
- If Layer 0 shows "IntentionalProcess" elevated ‚Üí activate its Layer 1 children
- Example: IntentionalProcess ‚Üí Reasoning, Planning, Deception, Cooperation

**Layer 2** (~10,000 concepts): **Selectively active** - Mid-level concepts
- SUMO depth 5-6: Specific semantic categories
- Triggered by: Parent concepts in Layer 1
- Example: Deception ‚Üí misdirection, concealment, strategic_withholding

**Layer 3** (~50,000 concepts): **On-demand** - Specific detailed concepts
- SUMO depth 5-8 (remaining): Fine-grained instances
- Activated only for dominant Layer 2 concepts
- Handles disambiguation and edge cases

**Layer 4** (~20,000 concepts): **Rare/technical** - Deep introspection
- SUMO depth 4,9+: Uncommon terms and technical concepts
- Activated for specialized contexts

**Compute Example** (1,000 probe budget):
```
Base: 100 Layer 0 probes (always active)
Budget: 900 remaining probes

If conversation shows:
  - "IntentionalProcess": 0.85 (top-1)
  - "Communication": 0.72 (top-2)
  - "Artifact": 0.65 (top-3)

Then activate:
  - IntentionalProcess children (Layer 1): 300 probes
  - Communication children (Layer 1): 300 probes
  - Artifact children (Layer 1): 300 probes

If "Deception" (child of IntentionalProcess) dominates:
  - Sleep other IntentionalProcess children
  - Activate Deception's Layer 2 children
```

**Analogy**: Proprioception (continuous background) ‚Üí Interoception (triggered attention) ‚Üí Introspection (conscious analysis)

### Pipeline: KIF ‚Üí WordNet (COMPLETE)

**Problem**: SUMO.owl is "a provisional and necessarily lossy translation" (per file header). The OWL version had 3,202 orphan roots with Entity isolated at depth 0, providing only 4.6% coverage.

**Solution**: Parse authoritative Merge.kif directly.

**Step 1: Parse SUMO KIF Hierarchy** (‚úÖ Complete)
- Source: `data/concept_graph/sumo_source/Merge.kif`
- Extract `(subclass Child Parent)` relations
- Build directed graph and reverse for Entity‚Üíchildren traversal
- Result: 684 classes, 805 subclass relations, max depth=10
- Hierarchy: Entity(depth=0) ‚Üí Physical/Abstract(1) ‚Üí Object/Process/Attribute(2) ‚Üí Artifact/Motion/Region(3) ‚Üí ...

**Step 2: Parse WordNet‚ÜíSUMO Mappings** (‚úÖ Complete)
- Source: WordNetMappings30-{noun,verb,adj,adv}.txt
- Format: `synset_offset POS_code POS ... | definition &%SUMOTerm[+=]`
- Regex: Extract offset+POS from start, SUMO term from end
- Result: 105,042 WordNet‚ÜíSUMO mappings
- Coverage: 83,134 synsets successfully joined (79.1%)

**Step 3: Hierarchical Layer Assignment** (‚úÖ Complete)
- **Layer 0**: SUMO depth 2-3, sample top 2-3 per term ‚Üí 83 concepts
- **Layer 1**: SUMO depth 4, sample top 10 per term ‚Üí 878 concepts
- **Layer 2**: SUMO depth 5-6, sample top 30-60 per term ‚Üí 7,329 concepts
- **Layer 3**: SUMO depth 5-8 remaining + all depth 7-8 ‚Üí 48,641 concepts
- **Layer 4**: SUMO depth 4,9+ remaining ‚Üí 16,823 concepts
- **Total**: 73,754 concepts (88.7% coverage)

**Step 4: Hierarchical Activation Metadata** (‚úÖ Complete)
- Built SUMO child‚Üíparent mappings
- Each layer includes `activation.parent_synsets` listing which Layer N-1 concepts trigger it
- Enables selective probe activation based on parent concept activity

### Implementation

**Files**:
- `src/build_sumo_wordnet_layers.py`: Build SUMO hierarchy from KIF
- `src/build_abstraction_layers.py`: Assign hierarchical layers with activation metadata
- `data/concept_graph/abstraction_layers/layer{0-4}.json`: Final output

**Output Format**:
```json
{
  "metadata": {
    "layer": 1,
    "description": "Major semantic domains (activated by Layer 0 parent concepts)",
    "total_concepts": 878,
    "top_sumo_terms": {"Communication": 10, "Transportation": 10, ...},
    "activation": {
      "parent_layer": 0,
      "parent_synsets": ["process.n.01", "motion.n.01", ...],
      "activation_model": "hierarchical"
    }
  },
  "concepts": [...]
}
```

### Current Status

**Completed** ‚úÖ:
1. Fixed SUMO OWL ‚Üí switched to authoritative KIF
2. Parsed 105K WordNet‚ÜíSUMO mappings (79% coverage)
3. Built 5-layer hierarchy with hierarchical activation
4. Moved scripts to `src/` for production use
5. Added parent‚Üíchild mappings for selective activation
6. Documented hierarchical activation model in project plan

**Next Steps**:
1. Implement hierarchical probe activation system
2. Test adaptive compute allocation (1K probe budget)
3. Validate topic tracking via Layer 0 top-10
4. Measure efficiency gains vs flat probe architecture

### Expected Output

5 layer files with full semantic coverage:

```json
{
  "metadata": {
    "layer": 1,
    "description": "Most abstract (top-level SUMO entities)",
    "total_concepts": ~200,
    "sumo_terms": ["Entity", "Physical", "Abstract", "Process"],
    "pos_distribution": {"n": 120, "v": 50, "a": 20, "r": 10}
  },
  "concepts": [
    {
      "synset": "entity.n.01",
      "lemmas": ["entity", "something"],
      "definition": "that which is perceived or known...",
      "sumo_term": "Entity",
      "sumo_depth": 0,
      "layer": 1,
      "hypernyms": [],
      "hyponyms": ["physical_entity.n.01", "abstraction.n.06"],
      "similar_tos": [],
      "antonyms": []
    }
  ]
}
```

### Files

**Scripts**:
- `scripts/parse_sumo_kif.py` - Original KIF parser (partial, needs OWL support)
- `scripts/build_sumo_wordnet_layers.py` - OWL‚ÜíKIF‚ÜíWordNet pipeline (current)

**Data** (pending completion):
- `data/concept_graph/sumo_layers/layer1.json` ‚Üí `layer5.json`
- `data/concept_graph/sumo_hierarchy/sumo_hierarchy.json` (intermediate)
- `data/concept_graph/sumo_hierarchy/wordnet_sumo_mapping.json` (intermediate)

**Dependencies**:
- `rdflib` - OWL parsing
- `networkx` - Hierarchy graph traversal
- `nltk` + `wordnet` - Synset enrichment

### Timeline

**This Week**: Debug KIF parser, complete mapping join
**Next Week**: Generate 5-layer files, validate coverage
**Integration**: Feed into Phase 11 (production scale) concept library

---

**Status**: Planning evaluation improvements (Phases 3-6) to validate and improve training/steering quality. Building hierarchical semantic foundation for scaled concept library (Phase 8).

**Last Updated**: November 6, 2025
